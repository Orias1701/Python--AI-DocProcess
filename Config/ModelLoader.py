import os
import shutil
import torch
from sentence_transformers import SentenceTransformer

def CudaCheck():
    print("CUDA supported:", torch.cuda.is_available())
    print("Number of GPUs:", torch.cuda.device_count())
    if torch.cuda.is_available():
        print("Current GPU name:", torch.cuda.get_device_name(0))
        print("CUDA device capability:", torch.cuda.get_device_capability(0))
        print("CUDA version (PyTorch):", torch.version.cuda)
        print("cuDNN version:", torch.backends.cudnn.version())
    else:
        print("CUDA not available.")


def load_sentence_model(path_or_name, device):
    """Load model SentenceTransformer with fallback CPU if GPU error."""
    try:
        return SentenceTransformer(path_or_name, device=str(device))
    except (OSError, FileNotFoundError) as e:
        print("‚ùå Model files missing:", e)
        return None
    except RuntimeError as e:
        print("‚ö†Ô∏è GPU issue, fallback to CPU:", e)
        return SentenceTransformer(path_or_name, device="cpu")
    except Exception as e:
        print("‚ùå Unexpected error:", e)
        raise


def ensure_cached_model(model_name: str, cached_path: str):
    """
    T·∫£i model t·ª´ HuggingFace n·∫øu cache ch∆∞a c√≥.
    Chu·∫©n ho√° th√†nh c·∫•u tr√∫c SentenceTransformers h·ª£p l·ªá.
    """
    if not os.path.exists(cached_path):
        print(f"üì• Downloading and caching model to {cached_path} ...")
        model = SentenceTransformer(model_name)
        model.save(cached_path)
        print("‚úÖ Model cached successfully.")
    else:
        # Ki·ªÉm tra xem c·∫•u tr√∫c c√≥ h·ª£p l·ªá ch∆∞a
        cfg_file = os.path.join(cached_path, "config_sentence_transformers.json")
        if not os.path.exists(cfg_file):
            print("‚öôÔ∏è Rebuilding SentenceTransformers structure in cache...")
            tmp_model = SentenceTransformer(model_name)
            tmp_model.save(cached_path)
    return cached_path


def init_sentence_model(EMBEDD_MODEL: str, cached_path: str = None):
    """
    G·ªôp to√†n b·ªô logic:
    - Ki·ªÉm tra CUDA v√† ch·ªçn device
    - ∆Øu ti√™n load model t·ª´ cached_path (t·∫°o n·∫øu ch∆∞a c√≥)
    - T·ª± fallback CPU n·∫øu GPU kh√¥ng kh·∫£ d·ª•ng
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    print("\nüîç Checking CUDA and GPU status...")
    CudaCheck()

    # N·∫øu c√≥ cache path ‚Üí ƒë·∫£m b·∫£o h·ª£p l·ªá
    if cached_path:
        os.makedirs(cached_path, exist_ok=True)
        ensure_cached_model(EMBEDD_MODEL, cached_path)
        print(f"‚ÑπÔ∏è Sentence Transformer cached at: {cached_path}")
        model = load_sentence_model(cached_path, device)
    else:
        print(f"‚ÑπÔ∏è Sentence Transformer: {EMBEDD_MODEL}")
        model = load_sentence_model(EMBEDD_MODEL, device)

    print(f"‚úÖ Using device: {device}")
    return model, device
