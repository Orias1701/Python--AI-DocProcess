import re
import numpy as np

from underthesea import sent_tokenize

class ChunkUndertheseaBuilder:
    """
    B·ªô t√°ch vƒÉn b·∫£n ti·∫øng Vi·ªát th√¥ng minh:
      1Ô∏è‚É£ L·ªçc tr∆∞·ªõc (Extractive): ch·ªâ gi·ªØ c√°c c√¢u c√≥ √Ω ch√≠nh
      2Ô∏è‚É£ G·ªôp sau (Semantic): nh√≥m c√°c c√¢u tr·ªçng t√¢m theo ng·ªØ nghƒ©a
    """

    def __init__(self,
                 embedder,
                 device: str = "cpu",
                 min_words: int = 256,
                 max_words: int = 768,
                 sim_threshold: float = 0.7,
                 key_sent_ratio: float = 0.4):
        if embedder is None:
            raise ValueError("‚ùå C·∫ßn truy·ªÅn m√¥ h√¨nh embedder ƒë√£ load s·∫µn.")
        self.embedder = embedder
        self.device = device
        self.min_words = min_words
        self.max_words = max_words
        self.sim_threshold = sim_threshold
        self.key_sent_ratio = key_sent_ratio

    # ============================================================
    # 1Ô∏è‚É£ T√°ch c√¢u
    # ============================================================
    def _split_sentences(self, text: str):
        """T√°ch c√¢u ti·∫øng Vi·ªát (fallback n·∫øu underthesea l·ªói)."""
        text = re.sub(r"[\x00-\x1f]+", " ", text)
        try:
            sents = sent_tokenize(text)
        except Exception:
            sents = re.split(r"(?<=[.!?])\s+", text)
        return [s.strip() for s in sents if len(s.strip()) > 2]

    # ============================================================
    # 2Ô∏è‚É£ Encode an to√†n (GPU/CPU fallback)
    # ============================================================
    def _encode(self, sentences):
        try:
            return self.embedder.encode(
                sentences,
                convert_to_numpy=True,
                show_progress_bar=False,
                device=str(self.device)
            )
        except TypeError:
            return self.embedder.encode(sentences, convert_to_numpy=True, show_progress_bar=False)
        except RuntimeError as e:
            if "CUDA" in str(e):
                print("‚ö†Ô∏è GPU OOM, fallback sang CPU.")
                return self.embedder.encode(
                    sentences, convert_to_numpy=True, show_progress_bar=False, device="cpu"
                )
            raise e

    # ============================================================
    # 3Ô∏è‚É£ L·ªçc √Ω ch√≠nh tr∆∞·ªõc (EXTRACTIVE)
    # ============================================================
    def _extractive_filter(self, sentences):
        """Ch·ªçn ra top-k c√¢u ƒë·∫°i di·ªán n·ªôi dung nh·∫•t."""
        if len(sentences) <= 3:
            return sentences

        embeddings = self._encode(sentences)
        mean_vec = np.mean(embeddings, axis=0)
        sims = np.dot(embeddings, mean_vec) / (
            np.linalg.norm(embeddings, axis=1) * np.linalg.norm(mean_vec)
        )

        # Ch·ªçn top-k c√¢u c√≥ similarity cao nh·∫•t
        k = max(1, int(len(sentences) * self.key_sent_ratio))
        idx = np.argsort(-sims)[:k]
        idx.sort()  # gi·ªØ th·ª© t·ª± g·ªëc
        selected = [sentences[i] for i in idx]
        return selected

    # ============================================================
    # 4Ô∏è‚É£ G·ªôp c√°c c√¢u tr·ªçng t√¢m theo ng·ªØ nghƒ©a
    # ============================================================
    def _semantic_group(self, sentences):
        """G·ªôp c√°c c√¢u ƒë√£ l·ªçc theo m·ª©c t∆∞∆°ng ƒë·ªìng ng·ªØ nghƒ©a."""
        if not sentences:
            return []

        embeddings = self._encode(sentences)
        embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)

        chunks, cur_chunk, cur_len = [], [], 0
        for i, sent in enumerate(sentences):
            wc = len(sent.split())
            if not cur_chunk:
                cur_chunk.append(sent)
                cur_len = wc
                continue

            sim = np.dot(embeddings[i - 1], embeddings[i])
            too_long = cur_len + wc > self.max_words
            too_short = cur_len < self.min_words
            topic_changed = sim < self.sim_threshold

            if too_long or (not too_short and topic_changed):
                chunks.append(" ".join(cur_chunk))
                cur_chunk = [sent]
                cur_len = wc
            else:
                cur_chunk.append(sent)
                cur_len += wc

        if cur_chunk:
            chunks.append(" ".join(cur_chunk))
        return chunks

    # ============================================================
    # 5Ô∏è‚É£ H√†m ch√≠nh build()
    # ============================================================
    def build(self, full_text: str):
        """
        Tr·∫£ v·ªÅ list ch·ª©a {Index, Content} cho t·ª´ng chunk.
        Quy tr√¨nh:
            - L·ªçc c√¢u tr·ªçng t√¢m tr∆∞·ªõc
            - G·ªôp c√°c c√¢u ƒë√£ l·ªçc theo ng·ªØ nghƒ©a
        """
        all_sentences = self._split_sentences(full_text)
        print(f"üìÑ T·ªïng s·ªë c√¢u: {len(all_sentences)}")

        # --- B∆∞·ªõc 1: l·ªçc √Ω ch√≠nh ---
        filtered = self._extractive_filter(all_sentences)
        print(f"‚ú® Gi·ªØ l·∫°i {len(filtered)} c√¢u (~{len(filtered)/len(all_sentences):.0%}) sau extractive filter")

        # --- B∆∞·ªõc 2: g·ªôp th√†nh c√°c ƒëo·∫°n ng·ªØ nghƒ©a ---
        chunks = self._semantic_group(filtered)
        results = [{"Index": i, "Content": chunk} for i, chunk in enumerate(chunks, start=1)]

        print(f"üîπ T·∫°o {len(results)} chunk ng·ªØ nghƒ©a t·ª´ {len(filtered)} c√¢u tr·ªçng t√¢m.")
        return results
