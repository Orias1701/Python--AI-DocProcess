import torch
from typing import List, Dict
from . import Json_ChunkUnder


class RecursiveSummarizer:
    """
    Bá»™ tÃ³m táº¯t há»c thuáº­t tiáº¿ng Viá»‡t theo hÆ°á»›ng:
    Extractive (chunk semantic) + Abstractive (recursive summarization)
    """

    def __init__(
        self,
        tokenizer,
        summarizer,
        sum_device: str,
        chunk_builder: Json_ChunkUnder.ChunkUndertheseaBuilder,
        max_length: int = 256,
        min_length: int = 64,
        max_depth: int = 5
    ):
        """
        tokenizer: AutoTokenizer Ä‘Ã£ load sáºµn.
        summarizer: AutoModelForSeq2SeqLM (ViT5 / BartPho / mT5)
        sum_device: 'cuda' hoáº·c 'cpu'
        chunk_builder: ChunkUndertheseaBuilder instance.
        """
        self.tokenizer = tokenizer
        self.model = summarizer
        self.device = sum_device
        self.chunk_builder = chunk_builder
        self.max_length = max_length
        self.min_length = min_length
        self.max_depth = max_depth

    # ============================================================
    # 1ï¸âƒ£ HÃ m tÃ³m táº¯t 1 Ä‘oáº¡n
    # ============================================================
    def summarize_single(self, text: str) -> str:
        """
        TÃ³m táº¯t 1 Ä‘oáº¡n Ä‘Æ¡n báº±ng mÃ´ hÃ¬nh abstractive (ViT5/BartPho).
        """
        if not text or len(text.strip()) == 0:
            return ""

        if "vit5" in str(self.model.__class__).lower():
            input_text = f"vietnews: {text.strip()} </s>"
        else:
            input_text = text.strip()

        try:
            inputs = self.tokenizer(
                input_text,
                return_tensors="pt",
                truncation=True,
                max_length=1024
            ).to(self.device)

            with torch.no_grad():
                summary_ids = self.model.generate(
                    **inputs,
                    max_length=self.max_length,
                    min_length=self.min_length,
                    num_beams=4,
                    no_repeat_ngram_size=3,
                    early_stopping=True
                )

            summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)
            return summary.strip()

        except torch.cuda.OutOfMemoryError:
            print("âš ï¸ GPU OOM â€“ fallback sang CPU.")
            self.model = self.model.to("cpu")
            inputs = inputs.to("cpu")

            with torch.no_grad():
                summary_ids = self.model.generate(
                    **inputs,
                    max_length=self.max_length,
                    min_length=self.min_length,
                    num_beams=4
                )

            return self.tokenizer.decode(summary_ids[0], skip_special_tokens=True).strip()

        except Exception as e:
            print(f"âŒ Lá»—i khi tÃ³m táº¯t Ä‘oáº¡n: {e}")
            return ""

    # ============================================================
    # 2ï¸âƒ£ Äá»‡ quy tÃ³m táº¯t vÄƒn báº£n dÃ i
    # ============================================================
    def summarize_recursive(self, text: str, depth: int = 0, minInput: int = 256, maxInput: int = 1024) -> str:
        """
        Äá»‡ quy tÃ³m táº¯t vÄƒn báº£n dÃ i:
        - <256 tá»«: giá»¯ nguyÃªn
        - <1024 tá»«: tÃ³m táº¯t trá»±c tiáº¿p
        - >=1024 tá»«: chia chunk + tÃ³m táº¯t tá»«ng pháº§n â†’ gá»™p â†’ Ä‘á»‡ quy
        """
        word_count = len(text.split())
        indent = "  " * depth
        print(f"{indent}ğŸ”¹ Level {depth}: {word_count} tá»«")

        # 1ï¸âƒ£ VÄƒn báº£n ngáº¯n
        if word_count < minInput:
            return self.summarize_single(text)

        else:
            chunks = self.chunk_builder.build(text)
            summaries = []

            for item in chunks:
                content = item.get("Content", "")
                print(content)
                idx = item.get("Index", "?")
                wc = len(content.split())

                if wc < 20:
                    print(f"{indent}âš ï¸ Bá» qua chunk {idx} (quÃ¡ ngáº¯n)")
                    continue

                print(f"{indent}ğŸ”¸ Chunk {idx}: {wc} tá»«")
                sub_summary = self.summarize_single(content)
                if sub_summary:
                    summaries.append(sub_summary)

            merged_summary = "\n".join(summaries)
            merged_len = len(merged_summary.split())
            print(f"{indent}ğŸ” Gá»™p {len(summaries)} summary â†’ {merged_len} tá»«")

            # Äá»‡ quy náº¿u váº«n dÃ i
            if merged_len > 1024 and depth < self.max_depth:
                return self.summarize_recursive(merged_summary, depth + 1)
            else:
                return merged_summary

    # ============================================================
    # 3ï¸âƒ£ HÃ m chÃ­nh cho ngÆ°á»i dÃ¹ng
    # ============================================================
    def summarize(self, full_text: str, minInput: int = 256, maxInput: int = 1024) -> Dict[str, str]:
        """
        Giao diá»‡n chÃ­nh:
        - Nháº­n text dÃ i
        - Tá»± Ä‘á»™ng chia chunk, tÃ³m táº¯t, gá»™p
        - Tráº£ vá» dict gá»“m summary vÃ  thá»‘ng kÃª
        """
        original_len = len(full_text.split())
        summary = self.summarize_recursive(full_text, depth = 0, minInput = minInput, maxInput = maxInput)

        summary_len = len(summary.split())
        ratio = round(summary_len / original_len, 3) if original_len else 0

        print(f"\nâœ¨ FINAL SUMMARY ({summary_len}/{original_len} tá»«, r={ratio}) âœ¨")
        return {
            "summary_text": summary,
            "original_words": original_len,
            "summary_words": summary_len,
            "compression_ratio": ratio
        }
