{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133581e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import faiss\n",
    "import logging\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from Libraries import A0_MyUtils as A0, A1_TextProcess as A1, A2_PdfProcess as A2\n",
    "from Libraries import B1_ExtractData as B1, B2_MergeData as B2, B3_GetStructures as B3\n",
    "from Libraries import B4_ChunkMaster as B4, B5_ChunkFlex as B5, B6_ChunkFixed as B6\n",
    "from Libraries import C1_CreateSchema as C1, C2_Embedding as C2, C3_CheckConstruct as C3\n",
    "from Libraries import D0_FaissConvert as D0, D1_Search as D1, D2_Rerank as D2, D3_Respond as D3\n",
    "from Config import Widgets, Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a8e42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets_list = Widgets.create_name_form()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2337c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "force_download = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cd76cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Configs.WidgetValues(widgets_list)\n",
    "\n",
    "data_foler = config[\"data_folder\"]\n",
    "dcmt_path = config[\"dcmt_path\"]\n",
    "base_folder = config[\"base_folder\"]\n",
    "base_path = config[\"base_path\"]\n",
    "extracted_path = config[\"extracted_path\"]\n",
    "merged_path = config[\"merged_path\"]\n",
    "struct_path = config[\"struct_path\"]\n",
    "chunks_base = config[\"chunks_base\"]\n",
    "chunks_segment = config[\"chunks_segment\"]\n",
    "schema_ex_path = config[\"schema_ex_path\"]\n",
    "embedding_path = config[\"embedding_path\"]\n",
    "torch_path = config[\"torch_path\"]\n",
    "faiss_path = config[\"faiss_path\"]\n",
    "mapping_path = config[\"mapping_path\"]\n",
    "mapping_data = config[\"mapping_data\"]\n",
    "\n",
    "FILE_TYPE = config[\"FILE_TYPE\"]\n",
    "DATA_KEY = config[\"DATA_KEY\"]\n",
    "EMBE_KEY = config[\"EMBE_KEY\"]\n",
    "SWITCH = config[\"SWITCH\"]\n",
    "EMBEDD_MODEL = config[\"EMBEDD_MODEL\"]\n",
    "SEARCH_EGINE = config[\"SEARCH_EGINE\"]\n",
    "RERANK_MODEL = config[\"RERANK_MODEL\"]\n",
    "RESPON_MODEL = config[\"RESPON_MODEL\"]\n",
    "MERGE = config[\"MERGE\"]\n",
    "API_KEY = config[\"API_KEY\"]\n",
    "\n",
    "WORD_LIMIT = config[\"WORD_LIMIT\"]\n",
    "LEVEL_INPUT = config[\"LEVEL_INPUT\"]\n",
    "LEVEL_VALUES = config[\"LEVEL_VALUES\"]\n",
    "\n",
    "Contents = LEVEL_VALUES[-1] if LEVEL_VALUES else None\n",
    "\n",
    "SEARCH_ENGINE = faiss.IndexFlatIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0896a406",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CUDA supported:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current GPU name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA device capability:\", torch.cuda.get_device_capability(0))\n",
    "    print(\"CUDA version (PyTorch):\", torch.version.cuda)\n",
    "    print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "else:\n",
    "    print(\"CUDA not available.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cached_path = \"../Models\"\n",
    "\n",
    "def load_auto_model(path_or_name, device):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(path_or_name)\n",
    "        model = AutoModel.from_pretrained(path_or_name).to(device)\n",
    "        return tokenizer, model\n",
    "    except (OSError, FileNotFoundError) as e:\n",
    "        print(\"❌ Model files missing:\", e)\n",
    "        return None, None\n",
    "    except RuntimeError as e:\n",
    "        print(\"⚠️ GPU issue, fallback to CPU:\", e)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(path_or_name)\n",
    "        model = AutoModel.from_pretrained(path_or_name).to(\"cpu\")\n",
    "        return tokenizer, model\n",
    "    except Exception as e:\n",
    "        print(\"❌ Unexpected error:\", e)\n",
    "        raise\n",
    "\n",
    "def load_sentence_model(path_or_name, device):\n",
    "    try:\n",
    "        return SentenceTransformer(path_or_name, device=str(device))\n",
    "    except (OSError, FileNotFoundError) as e:\n",
    "        print(\"❌ Model files missing:\", e)\n",
    "        return None\n",
    "    except RuntimeError as e:\n",
    "        print(\"⚠️ GPU issue, fallback to CPU:\", e)\n",
    "        return SentenceTransformer(path_or_name, device=\"cpu\")\n",
    "    except Exception as e:\n",
    "        print(\"❌ Unexpected error:\", e)\n",
    "        raise\n",
    "\n",
    "\n",
    "# ================= Main logic =================\n",
    "if SWITCH == \"Auto Model\":\n",
    "    if os.path.exists(cached_path):\n",
    "        tokenizer, model = load_auto_model(cached_path, device)\n",
    "        print(f\"ℹ️ Auto Model: {cached_path}\")\n",
    "        if model is None:\n",
    "            tokenizer, model = load_auto_model(EMBEDD_MODEL, device)\n",
    "    else:\n",
    "        print(f\"ℹ️ Auto Model: {EMBEDD_MODEL}\")\n",
    "        tokenizer, model = load_auto_model(EMBEDD_MODEL, device)\n",
    "\n",
    "elif SWITCH == \"Sentence Transformer\":\n",
    "    if os.path.exists(cached_path):\n",
    "        model = load_sentence_model(cached_path, device)\n",
    "        print(f\"ℹ️ Sentece Transformer: {cached_path}\")\n",
    "\n",
    "        if model is None:\n",
    "            model = load_sentence_model(EMBEDD_MODEL, device)\n",
    "    else:\n",
    "        print(f\"ℹ️ Sentece Transformer: {EMBEDD_MODEL}\")\n",
    "        model = load_sentence_model(EMBEDD_MODEL, device)\n",
    "\n",
    "print(f\"✅ Using: {device}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
