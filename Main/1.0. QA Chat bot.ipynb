{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "import torch\n",
    "import logging\n",
    "from typing import List\n",
    "from typing import Any, Dict\n",
    "from sentence_transformers import util\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from RAGLibrary import Widgets, Define\n",
    "from RAGLibrary import CheckConstruct, CreateSchema, FaissConvert, Embedding, Search, Rerank, Respond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611422ff2a07443a8c011f9e4493fd4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(description='File:  ', index=1, layout=Layout(width='33%'), options=('Harvard_Regulati…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794aef4012b34448b72433ebe7f5d73c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='contents', description='Data Key: ', layout=Layout(width='50%'), placeholder='Defau…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fbf24b3205b4a9cb6c725d61bfb0cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(description='Model: ', index=1, layout=Layout(width='50%'), options=('Auto Model', 'Se…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16dd03788ad442899f25938b720c9d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Embedder: ', index=2, layout=Layout(width='90%'), options=('vinai/phobert-base', 'keepit…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f168b51e3c28481b8612c9e666eedf5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Searcher: ', index=1, layout=Layout(width='90%'), options=('faiss.IndexHNSWFlat', 'faiss…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1893072e2c0b4f37884085ae60821143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Reranker: ', layout=Layout(width='90%'), options=('BAAI/bge-reranker-base',), value='BAA…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556bcd6199124651acd9855a74e98058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Response: ', layout=Layout(width='90%'), options=('gemini-2.0-flash-exp', 'vinai/PhoGPT-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440b80a2597d4ee0b0b6d0148c2fd821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='API Key:', index=4, layout=Layout(width='90%'), options=('AIzaSyDaHS-8h6GJkyVPhoX4svvYeB…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcabc7760fbf4c3dadab054672130385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(description='Max Level: ', index=4, layout=Layout(width='50%'), options=('0', '1', '2'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a611bf942144cc9bb8ffc8aeee7251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8efa3c0f0f241ecb6ed264546ee1399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='success', description='Save State', style=ButtonStyle()), Button(button_st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widgets_list = Widgets.create_name_form()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "force_download = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedder: VoVanPhuc/sup-SimCSE-VietNamese-phobert-base\n",
      "Searcher: faiss.IndexFlatIP\n",
      "Reranker: BAAI/bge-reranker-base\n",
      "Responer: gemini-2.0-flash-exp\n",
      "Data Key: contents\n",
      "Embe Key: embeddings\n",
      "Dcment  : ../Doc/HNMU_Regulations.pdf\n",
      "Chunked : ../Data/HNMU_Regulations/QA_HNMU_Regulations_Chunks.json\n",
      "Database: ../Data/HNMU_Regulations/QA_HNMU_Regulations_Database.json\n",
      "Torch   : ../Data/HNMU_Regulations/QA_HNMU_Regulations_Embeds_Merge.pt\n",
      "Faiss   : ../Data/HNMU_Regulations/QA_HNMU_Regulations_Embeds_Merge.faiss\n",
      "Mapping : ../Data/HNMU_Regulations/QA_HNMU_Regulations_Embeds_Merge_mapping.json\n",
      "Map Data: ../Data/HNMU_Regulations/QA_HNMU_Regulations_Embeds_Merge_map_data.json\n",
      "Schema  : ../Data/HNMU_Regulations/QA_HNMU_Regulations_Schema.json\n",
      "Model   : Sentence Transformer\n",
      "Merge   : no_Merge\n",
      "API Key : AIzaSyBPjyMfHkS9OW3h7G0kmLSQkWQMfqfX5v0\n",
      "Word    : 200\n",
      "Level   : 4\n",
      "Level Values: ['Chương', 'Điều', 'Khoản', 'Nội dung']\n"
     ]
    }
   ],
   "source": [
    "config = Define.WidgetValues(widgets_list)\n",
    "\n",
    "dcmt_path = config[\"dcmt_path\"]\n",
    "base_folder = config[\"base_folder\"]\n",
    "base_path = config[\"base_path\"]\n",
    "chunks_base = config[\"chunks_base\"]\n",
    "json_file_path = config[\"json_file_path\"]\n",
    "schema_ex_path = config[\"schema_ex_path\"]\n",
    "embedding_path = config[\"embedding_path\"]\n",
    "torch_path = config[\"torch_path\"]\n",
    "faiss_path = config[\"faiss_path\"]\n",
    "mapping_path = config[\"mapping_path\"]\n",
    "mapping_data = config[\"mapping_data\"]\n",
    "\n",
    "FILE_TYPE = config[\"FILE_TYPE\"]\n",
    "DATA_KEY = config[\"DATA_KEY\"]\n",
    "EMBE_KEY = config[\"EMBE_KEY\"]\n",
    "SWITCH = config[\"SWITCH\"]\n",
    "EMBEDD_MODEL = config[\"EMBEDD_MODEL\"]\n",
    "SEARCH_EGINE = config[\"SEARCH_EGINE\"]\n",
    "RERANK_MODEL = config[\"RERANK_MODEL\"]\n",
    "RESPON_MODEL = config[\"RESPON_MODEL\"]\n",
    "MERGE = config[\"MERGE\"]\n",
    "API_KEY = config[\"API_KEY\"]\n",
    "\n",
    "WORD_LIMIT = config[\"WORD_LIMIT\"]\n",
    "LEVEL_INPUT = config[\"LEVEL_INPUT\"]\n",
    "LEVEL_VALUES = config[\"LEVEL_VALUES\"]\n",
    "\n",
    "SEARCH_ENGINE = faiss.IndexFlatIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformer loaded successfully\n",
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if (SWITCH == \"Auto Model\"):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(EMBEDD_MODEL, force_download=force_download)\n",
    "        model = AutoModel.from_pretrained(EMBEDD_MODEL, force_download=force_download)\n",
    "        model = model.to(device)\n",
    "        print(\"Model and tokenizer loaded successfully\")\n",
    "    except Exception as e:\n",
    "        raise\n",
    "elif (SWITCH == \"Sentence Transformer\"):\n",
    "    try:\n",
    "        # model = SentenceTransformer(EMBEDD_MODEL).to(device)\n",
    "        model = SentenceTransformer(\"../../cached_model\")\n",
    "        print(\"SentenceTransformer loaded successfully\")\n",
    "    except Exception as e:\n",
    "        raise\n",
    "\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PREPROCESS TEXT \"\"\"\n",
    "\n",
    "def preprocess_text(text):\n",
    "    import re\n",
    "    if isinstance(text, list):\n",
    "        return [preprocess_text(t) for t in text]\n",
    "    if isinstance(text, str):\n",
    "        text = text.strip()\n",
    "        text = re.sub(r'[^\\w\\s\\(\\)\\.\\,\\;\\:\\-–]', '', text)\n",
    "        text = re.sub(r'[ ]{2,}', ' ', text)\n",
    "        return text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" CREATE EMBEDDING \"\"\"\n",
    "\n",
    "def create_embedding(texts, batch_size=32):\n",
    "    try:\n",
    "        embeddings = model.encode(texts, batch_size=batch_size, convert_to_tensor=True, device=device)\n",
    "        return embeddings\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(\"VRAM overflow. Switching to CPU.\")\n",
    "            model.to(\"cpu\")\n",
    "            return model.encode(texts, batch_size=batch_size, convert_to_tensor=True, device=\"cpu\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadEmbedding(embedding_path: str, device, DATA_KEY: str = \"content\", EMBE_KEY: str = \"data_embeddings\", field_keys: List[str] = [\"Câu hỏi\", \"Câu trả lời\", \"Câu hỏi Embedding\"]) -> Dict[str, Any]:\n",
    "    result = {}\n",
    "    print(f\"\\nĐang tải embedding từ {embedding_path}\\n\")\n",
    "    try:\n",
    "        data = torch.load(embedding_path, map_location=\"cpu\", weights_only=False)\n",
    "        \n",
    "        print(f\"Các key có sẵn: {list(data.keys())}\")\n",
    "\n",
    "        content = []\n",
    "        if isinstance(data, dict) and DATA_KEY in data:\n",
    "            content = data[DATA_KEY]\n",
    "            print(f\"Số mục trong '{DATA_KEY}': {len(content)}\")\n",
    "        else:\n",
    "            print(f\"Lỗi: File .pt không có key '{DATA_KEY}' hoặc không đúng định dạng.\")\n",
    "        \n",
    "        if not content:\n",
    "            print(\"Lỗi: File trống.\")\n",
    "        else:\n",
    "            for key in field_keys:\n",
    "                data_list = [item[key] for item in content if key in item]\n",
    "                if data_list:\n",
    "                    if key.lower().find(\"embedding\") != -1 and isinstance(data_list[0], (list, torch.Tensor)):\n",
    "                        result[key] = torch.tensor(data_list, dtype=torch.float32).to(device)\n",
    "                        print(f\"Đã tải '{key}' với kích thước: {result[key].shape}\")\n",
    "                    else:\n",
    "                        result[key] = data_list\n",
    "                        print(f\"Đã tải '{key}' với số mục: {len(data_list)}\")\n",
    "                else:\n",
    "                    print(f\"Cảnh báo: Không tìm thấy '{key}' trong '{DATA_KEY}'.\")\n",
    "                \n",
    "                if key not in result and key.lower().find(\"embedding\") != -1 and EMBE_KEY in data:\n",
    "                    embed_data = data[EMBE_KEY]\n",
    "                    if isinstance(embed_data, (list, torch.Tensor)) and len(embed_data) > 0:\n",
    "                        result[key] = torch.tensor(embed_data, dtype=torch.float32).to(device)\n",
    "                        print(f\"Đã tải '{key}' từ '{EMBE_KEY}' với kích thước: {result[key].shape}\")\n",
    "        \n",
    "        for key in field_keys:\n",
    "            if key in result:\n",
    "                if isinstance(result[key], torch.Tensor):\n",
    "                    print(f\"Số '{key}': {result[key].shape[0]}\")\n",
    "                else:\n",
    "                    print(f\"Số '{key}': {len(result[key])}\")\n",
    "            else:\n",
    "                print(f\"Lỗi: Không tải được '{key}'.\")\n",
    "        \n",
    "    except (KeyError, ValueError, RuntimeError) as e:\n",
    "        print(f\"Lỗi khi tải embedding: {e}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_questions = []\n",
    "qa_answers = []\n",
    "qa_question_embeddings = None\n",
    "required_fields = [\"Câu hỏi\", \"Câu trả lời\", \"Câu hỏi Embedding\"]\n",
    "\n",
    "data = {}\n",
    "if os.path.exists(embedding_path):\n",
    "    data = LoadEmbedding(\n",
    "        embedding_path=embedding_path,\n",
    "        device=device,\n",
    "        DATA_KEY=DATA_KEY,\n",
    "        EMBE_KEY=EMBE_KEY,\n",
    "        field_keys=required_fields\n",
    "    )\n",
    "\n",
    "if data:\n",
    "    print(\"\\nDữ liệu trả về:\")\n",
    "    for key in required_fields:\n",
    "        if key in data:\n",
    "            if isinstance(data[key], torch.Tensor):\n",
    "                print(f\"{key}: Tensor với kích thước {data[key].shape}\")\n",
    "            else:\n",
    "                print(f\"{key}: {len(data[key])} mục\")\n",
    "        else:\n",
    "            print(f\"Lỗi: Không tìm thấy '{key}' trong dữ liệu trả về.\")\n",
    "\n",
    "    qa_questions = data.get(\"Câu hỏi\", [])\n",
    "    qa_answers = data.get(\"Câu trả lời\", [])\n",
    "    qa_question_embeddings = data.get(\"Câu hỏi Embedding\")\n",
    "    if qa_question_embeddings is not None and isinstance(qa_question_embeddings, torch.Tensor):\n",
    "        print(f\"Kích thước của 'Câu hỏi Embedding': {qa_question_embeddings.shape}\")\n",
    "    else:\n",
    "        print(\"Lỗi: Không tìm thấy 'Câu hỏi Embedding' trong dữ liệu trả về.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_cache = {}\n",
    "def find_best_answer(user_question):\n",
    "    user_question = preprocess_text(user_question)\n",
    "    if user_question in question_cache:\n",
    "        user_embedding = question_cache[user_question]\n",
    "    else:\n",
    "        user_embedding = create_embedding([user_question])[0].to(device)\n",
    "        question_cache[user_question] = user_embedding\n",
    "    print(\"qa_question_embeddings:\", qa_question_embeddings)\n",
    "\n",
    "    similarities = util.pytorch_cos_sim(user_embedding, qa_question_embeddings)[0]\n",
    "    torch.cuda.empty_cache()\n",
    "    threshold = max(0.7, similarities.max().item() * 0.9)\n",
    "    matched_indices = torch.where(similarities >= threshold)[0]\n",
    "    \n",
    "    if len(matched_indices) > 0:\n",
    "        responses = sorted(\n",
    "            [(qa_answers[idx.item()], similarities[idx].item()) for idx in matched_indices],\n",
    "            key=lambda x: x[1], reverse=True\n",
    "        )[:5]\n",
    "        return responses\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<< Enter 'exit', 'quit', 'escape', 'bye' or Press ESC to exit >>\n",
      "Chatbot: Hello there! I'm here to help you =))\n",
      "qa_question_embeddings: None\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mChatbot: Goodbye!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m responses = \u001b[43mfind_best_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_input.strip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m responses:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mfind_best_answer\u001b[39m\u001b[34m(user_question)\u001b[39m\n\u001b[32m      8\u001b[39m     question_cache[user_question] = user_embedding\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mqa_question_embeddings:\u001b[39m\u001b[33m\"\u001b[39m, qa_question_embeddings)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m similarities = \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpytorch_cos_sim\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqa_question_embeddings\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     12\u001b[39m torch.cuda.empty_cache()\n\u001b[32m     13\u001b[39m threshold = \u001b[38;5;28mmax\u001b[39m(\u001b[32m0.7\u001b[39m, similarities.max().item() * \u001b[32m0.9\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Orias.ASUS\\miniconda3\\envs\\rag\\Lib\\site-packages\\sentence_transformers\\util.py:89\u001b[39m, in \u001b[36mpytorch_cos_sim\u001b[39m\u001b[34m(a, b)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpytorch_cos_sim\u001b[39m(a: Tensor, b: Tensor) -> Tensor:\n\u001b[32m     79\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[33;03m    Computes the cosine similarity between two tensors.\u001b[39;00m\n\u001b[32m     81\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     87\u001b[39m \u001b[33;03m        Tensor: Matrix with res[i][j] = cos_sim(a[i], b[j])\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcos_sim\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Orias.ASUS\\miniconda3\\envs\\rag\\Lib\\site-packages\\sentence_transformers\\util.py:104\u001b[39m, in \u001b[36mcos_sim\u001b[39m\u001b[34m(a, b)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[33;03mComputes the cosine similarity between two tensors.\u001b[39;00m\n\u001b[32m     95\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    101\u001b[39m \u001b[33;03m    Tensor: Matrix with res[i][j] = cos_sim(a[i], b[j])\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    103\u001b[39m a = _convert_to_batch_tensor(a)\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m b = \u001b[43m_convert_to_batch_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m a_norm = normalize_embeddings(a)\n\u001b[32m    107\u001b[39m b_norm = normalize_embeddings(b)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Orias.ASUS\\miniconda3\\envs\\rag\\Lib\\site-packages\\sentence_transformers\\util.py:73\u001b[39m, in \u001b[36m_convert_to_batch_tensor\u001b[39m\u001b[34m(a)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_convert_to_batch_tensor\u001b[39m(a: \u001b[38;5;28mlist\u001b[39m | np.ndarray | Tensor) -> Tensor:\n\u001b[32m     64\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[33;03m    Converts the input data to a tensor with a batch dimension.\u001b[39;00m\n\u001b[32m     66\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     71\u001b[39m \u001b[33;03m        Tensor: The converted tensor with a batch dimension.\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     a = \u001b[43m_convert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m     a = _convert_to_batch(a)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Orias.ASUS\\miniconda3\\envs\\rag\\Lib\\site-packages\\sentence_transformers\\util.py:44\u001b[39m, in \u001b[36m_convert_to_tensor\u001b[39m\u001b[34m(a)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[33;03mConverts the input `a` to a PyTorch tensor if it is not already a tensor.\u001b[39;00m\n\u001b[32m     36\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     41\u001b[39m \u001b[33;03m    Tensor: The converted tensor.\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, Tensor):\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     a = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[31mRuntimeError\u001b[39m: Could not infer dtype of NoneType"
     ]
    }
   ],
   "source": [
    "print(\"<< Enter 'exit', 'quit', 'escape', 'bye' or Press ESC to exit >>\")\n",
    "print(\"Chatbot: Hello there! I'm here to help you =))\")\n",
    "\n",
    "user_inputs = [\n",
    "    \"Sinh viên có thể được thi lại bao nhiêu lần?\",\n",
    "    \"Sinh viên chưa đăng ki học được trên cổng thông tin thì có thể học bổ sung không\",\n",
    "]\n",
    "\n",
    "i = 0\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        if i >= len(user_inputs):\n",
    "            user_input = \"exit\"\n",
    "        else:\n",
    "            user_input = user_inputs[i]\n",
    "    \n",
    "        # user_input = input(\"You: \")\n",
    "        if user_input.strip().lower() in [\"exit\", \"quit\", \"escape\", \"bye\", \"\"]:\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        responses = find_best_answer(user_input)\n",
    "\n",
    "        print(f\"You: {user_input.strip()}\")\n",
    "        if responses:\n",
    "            print(\"Chatbot:\")\n",
    "            for i, (response, score) in enumerate(responses, 1):\n",
    "                print(f\"{i}. [{score:.4f}] {response}\")\n",
    "        else:\n",
    "            print(\"Chatbot: Sorry I don't know the answer to that question =))\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nChatbot: Goodbye!\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
