{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307a6271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RAGLibrary import myWidgets, myRAG, checkConstruct, createSchema, faissConvert, embedding\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import faiss\n",
    "import logging\n",
    "from typing import Any, Dict\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5492c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets_list = myWidgets.create_name_form()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bff93f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "force_download = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426e75ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" DEFINE \"\"\"\n",
    "\n",
    "data   = widgets_list[0] #HBox 1\n",
    "keys   = widgets_list[1] #HBox 2\n",
    "choose = widgets_list[2] #HBox 3\n",
    "\n",
    "embedd_model = widgets_list[3]\n",
    "search_egine = widgets_list[4]\n",
    "rerank_model = widgets_list[5]\n",
    "respon_model = widgets_list[6]\n",
    "API_drop     = widgets_list[7]\n",
    "button_box   = widgets_list[8]\n",
    "\n",
    "# HBox 1\n",
    "file_name = data.children[0]\n",
    "file_type = data.children[1]\n",
    "\n",
    "# HBox 2\n",
    "data_key = keys.children[0]\n",
    "embe_key = keys.children[1]\n",
    "\n",
    "# HBox 3\n",
    "switch_model = choose.children[0]\n",
    "merge_otp    = choose.children[1]\n",
    "path_end_val = choose.children[1]\n",
    "\n",
    "# Get value\n",
    "data_folder   = file_name.value\n",
    "file_type_val = file_type.value\n",
    "\n",
    "data_key_val  = data_key.value\n",
    "embe_key_val  = embe_key.value\n",
    "\n",
    "API_key_val = API_drop.value\n",
    "switch      = switch_model.value\n",
    "merge       = merge_otp.value\n",
    "path_end    = path_end_val.value\n",
    "\n",
    "embedding_model = embedd_model.value\n",
    "searching_egine = search_egine.value\n",
    "reranking_model = rerank_model.value\n",
    "responing_model = respon_model.value\n",
    "\n",
    "\n",
    "# Define\n",
    "base_path = f\"../Data/{data_folder}/{file_type_val}_{data_folder}\"\n",
    "\n",
    "json_file_path = f\"{base_path}_Database.json\"\n",
    "schema_ex_path = f\"{base_path}_Schema.json\"\n",
    "embedding_path = f\"{base_path}_Embeds_{merge}\"\n",
    "\n",
    "torch_path  = f\"{embedding_path}.pt\"\n",
    "faiss_path  = f\"{embedding_path}.faiss\"\n",
    "mapping_path = f\"{embedding_path}_mapping.json\"\n",
    "mapping_data = f\"{embedding_path}_map_data.json\"\n",
    "\n",
    "FILE_TYPE    = file_type_val\n",
    "DATA_KEY     = data_key_val\n",
    "EMBE_KEY     = embe_key_val\n",
    "SWITCH       = switch\n",
    "EMBEDD_MODEL = embedding_model\n",
    "SEARCH_EGINE = searching_egine\n",
    "RERANK_MODEL = reranking_model\n",
    "RESPON_MODEL = responing_model\n",
    "\n",
    "if FILE_TYPE == \"Data\":\n",
    "    MERGE = merge\n",
    "else: \n",
    "    MERGE = \"no_Merge\"\n",
    "\n",
    "API_KEY = API_key_val\n",
    "\n",
    "SEARCH_ENGINE = faiss.IndexFlatIP\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"Embedder: {EMBEDD_MODEL}\")\n",
    "print(f\"Searcher: {SEARCH_EGINE}\")\n",
    "print(f\"Reranker: {RERANK_MODEL}\")\n",
    "print(f\"Responer: {RESPON_MODEL}\")\n",
    "print(f\"Data Key: {DATA_KEY}\")\n",
    "print(f\"Embe Key: {EMBE_KEY}\")\n",
    "print(f\"Database: {json_file_path}\")\n",
    "print(f\"Torch   : {torch_path}\")\n",
    "print(f\"Faiss   : {faiss_path}\")\n",
    "print(f\"Mapping : {mapping_path}\")\n",
    "print(f\"Map Data: {mapping_data}\")\n",
    "print(f\"Schema  : {schema_ex_path}\")\n",
    "print(f\"Model   : {SWITCH}\")\n",
    "print(f\"Merge   : {MERGE}\")\n",
    "print(f\"API Key : {API_KEY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765c5342",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if (SWITCH == \"Auto Model\"):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(EMBEDD_MODEL, force_download=force_download)\n",
    "        model = AutoModel.from_pretrained(EMBEDD_MODEL, force_download=force_download)\n",
    "        model = model.to(device)\n",
    "        print(\"Model and tokenizer loaded successfully\")\n",
    "    except Exception as e:\n",
    "        raise\n",
    "elif (SWITCH == \"Sentence Transformer\"):\n",
    "    try:\n",
    "        # model = SentenceTransformer(EMBEDD_MODEL).to(device)\n",
    "        model = SentenceTransformer(\"../../cached_model\")\n",
    "        print(\"SentenceTransformer loaded successfully\")\n",
    "    except Exception as e:\n",
    "        raise\n",
    "\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec9b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PREPROCESS TEXT \"\"\"\n",
    "\n",
    "def preprocess_text(text):\n",
    "    import re\n",
    "    if isinstance(text, list):\n",
    "        return [preprocess_text(t) for t in text]\n",
    "    if isinstance(text, str):\n",
    "        text = text.strip()\n",
    "        text = re.sub(r'[^\\w\\s\\(\\)\\.\\,\\;\\:\\-–]', '', text)\n",
    "        text = re.sub(r'[ ]{2,}', ' ', text)\n",
    "        return text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc325ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PREPROCESS DATA \"\"\"\n",
    "\n",
    "def preprocess_data(data):\n",
    "    if isinstance(data, dict):\n",
    "        return {key: preprocess_data(value) for key, value in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [preprocess_data(item) for item in data]\n",
    "    else:\n",
    "        return preprocess_text(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f693e794",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" LOAD SCHEMA \"\"\"\n",
    "\n",
    "def load_schema(schema_path: str) -> Dict[str, str]:\n",
    "    try:\n",
    "        with open(schema_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Schema file not found: {schema_path}\")\n",
    "        return {}\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Invalid schema file: {schema_path}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34740e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" FLATTEN JSON \"\"\"\n",
    "\n",
    "def flatten_json(data: Any, prefix: str = \"\", schema: Dict[str, str] = None) -> Dict[str, Any]:\n",
    "\n",
    "    flat = {}\n",
    "    \n",
    "    if schema is None:\n",
    "        schema = {}\n",
    "\n",
    "    # Nếu dữ liệu là dict\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            # Tạo tiền tố mới cho key\n",
    "            new_prefix = f\"{prefix}{key}\" if prefix else key\n",
    "\n",
    "            # Nếu là dict hoặc list, làm phẳng\n",
    "            if isinstance(value, (dict, list)):\n",
    "                flat.update(flatten_json(value, f\"{new_prefix}.\", schema))\n",
    "            else:\n",
    "                # Nếu là kiểu dữ liệu cơ bản, thêm vào dict\n",
    "                flat[new_prefix] = value\n",
    "\n",
    "    # Nếu là một danh sách và không rỗng\n",
    "    elif isinstance(data, list) and data:\n",
    "        # Lưu danh sách vào dictionary với key là tiền tố (bỏ dấu '.')\n",
    "        flat[prefix.rstrip('.')] = data\n",
    "\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a3d8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" CREATE EMBEDDDING \"\"\"\n",
    "\n",
    "def create_embedding(texts, batch_size=32):\n",
    "    try:\n",
    "        embeddings = model.encode(texts, batch_size=batch_size, convert_to_tensor=True, device=device)\n",
    "        return embeddings\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(\"VRAM overflow. Switching to CPU.\")\n",
    "            model.to(\"cpu\")\n",
    "            return model.encode(texts, batch_size=batch_size, convert_to_tensor=True, device=\"cpu\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f13c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" CREATE EMBEDDDINGS \"\"\"\n",
    "\n",
    "def create_embeddings(data: Any, schema: Dict[str, str], model, device: torch.device, merge: str = \"NO\") -> Dict[str, Any]:\n",
    "    flat_data = flatten_json(data, schema=schema)\n",
    "    embeddings = {}\n",
    "    \n",
    "    if MERGE == \"Merge\":\n",
    "        # Gộp tất cả các trường\n",
    "        merged_texts = []\n",
    "        for key, value in flat_data.items():\n",
    "            if schema.get(key) in [\"string\", \"array\"]:\n",
    "                if isinstance(value, str) and value.strip():\n",
    "                    merged_texts.append(preprocess_text(value))\n",
    "                elif isinstance(value, list):\n",
    "                    text = \"\\n\".join([preprocess_text(str(item)) for item in value if str(item).strip()])\n",
    "                    if text.strip():\n",
    "                        merged_texts.append(text)\n",
    "        if merged_texts:\n",
    "            # Tạo embedding\n",
    "            merged_text = \"\\n\".join(merged_texts)\n",
    "            if merged_text.strip():\n",
    "                embedding = create_embedding(merged_text).to(device)\n",
    "                embeddings[\"merged_embedding\"] = embedding\n",
    "    else:\n",
    "        # Embeddingriêng lẻ\n",
    "        for key, value in flat_data.items():\n",
    "            if schema.get(key) in [\"string\", \"array\"]:\n",
    "                if isinstance(value, str) and value.strip():\n",
    "                    embedding = create_embedding(preprocess_text(value)).to(device)\n",
    "                    embeddings[f\"{key} Embedding\"] = embedding\n",
    "                elif isinstance(value, list):\n",
    "                    text = \"\\n\".join([preprocess_text(str(item)) for item in value if str(item).strip()])\n",
    "                    if text.strip():\n",
    "                        embedding = create_embedding(text).to(device)\n",
    "                        embeddings[f\"{key} Embedding\"] = embedding\n",
    "\n",
    "    # Kết hợp dữ liệu gốc và embedding\n",
    "    if MERGE == \"Merge\":\n",
    "        result = [{} for _ in range(len(data))] if isinstance(data, list) else {}\n",
    "    else:\n",
    "        result = data.copy()\n",
    "\n",
    "    for embed_key, embed_value in embeddings.items():\n",
    "        if MERGE == \"Merge\":\n",
    "            result[\"Merged_text\"] = merged_text\n",
    "            result[\"Merged_embedding\"] = embed_value.tolist()\n",
    "        else:\n",
    "            keys = embed_key.split(\" Embedding\")[0].split('.')\n",
    "            current = result\n",
    "            for i, k in enumerate(keys[:-1]):\n",
    "                current = current.setdefault(k, {})\n",
    "            current[keys[-1] + \" Embedding\"] = embed_value.tolist()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc328d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" JSON EMBEDDDING \"\"\"\n",
    "\n",
    "def json_embeddings(json_file_path: str, \n",
    "                    torch_path: str, \n",
    "                    schema_path: str, \n",
    "                    model, \n",
    "                    device: torch.device, \n",
    "                    DATA_KEY: str, \n",
    "                    EMBE_KEY: str) -> None:\n",
    "    \n",
    "    # Kiểm tra nếu file embedding đã tồn tại\n",
    "    if os.path.exists(torch_path):\n",
    "        print(f\"\\nEmbedding loaded from {torch_path}\\n\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nCreating embeddings for JSON data...\\n\")\n",
    "    try:\n",
    "        # Đọc schema\n",
    "        schema = load_schema(schema_path)\n",
    "        if not schema:\n",
    "            raise ValueError(\"Schema is empty or invalid\")\n",
    "\n",
    "        # Đọc file JSON\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "            data_pairs = json.load(f)\n",
    "\n",
    "        if not isinstance(data_pairs, list):\n",
    "            data_pairs = [data_pairs]\n",
    "\n",
    "        # Xử lý từng bộ JSON\n",
    "        output_data = []\n",
    "        for data in data_pairs:\n",
    "            # Tiền xử lý văn bản\n",
    "            flat_data = flatten_json(data)\n",
    "            for key, value in flat_data.items():\n",
    "                if isinstance(value, (str, list)):\n",
    "                    flat_data[key] = preprocess_text(value)\n",
    "            \n",
    "            # Khôi phục cấu trúc gốc với dữ liệu đã tiền xử lý\n",
    "            processed_data = data.copy()\n",
    "            for key, value in flat_data.items():\n",
    "                keys = key.split('.')\n",
    "                current = processed_data\n",
    "                for k in keys[:-1]:\n",
    "                    current = current[k]\n",
    "                current[keys[-1]] = value\n",
    "\n",
    "            # Tạo embedding\n",
    "            result = create_embeddings(processed_data, schema, model, device)\n",
    "            output_data.append(result)\n",
    "\n",
    "        # Lưu embedding riêng vào file .pt\n",
    "        embeddings_only = []\n",
    "        # datas_only =[]\n",
    "        for item in output_data:\n",
    "            flat_item = flatten_json(item)\n",
    "            if MERGE == \"Merge\":\n",
    "                # data_dict = {k: v for k, v in flat_item.items() if not k == \"Merged_embedding\"}\n",
    "                embed_dict = {k: v for k, v in flat_item.items() if k == \"Merged_embedding\"}\n",
    "            else:\n",
    "                # data_dict = {k: v for k, v in flat_item.items() if not k.endswith(\"Embedding\")}\n",
    "                embed_dict = {k: v for k, v in flat_item.items() if k.endswith(\"Embedding\")}\n",
    "            # datas_only.append(data_dict)\n",
    "            embeddings_only.append(embed_dict)\n",
    "\n",
    "\n",
    "        torch.save({\n",
    "            f\"{DATA_KEY}\": output_data,\n",
    "            f\"{EMBE_KEY}\": embeddings_only\n",
    "        }, torch_path)\n",
    "\n",
    "        print(f\"Embedding tensor saved to {torch_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing JSON with embeddings: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3db5bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\" MAIN - CREATE SCHEMA & EMBEDDDING \"\"\"\"\"\n",
    "\n",
    "if os.path.exists(json_file_path):\n",
    "    json_embeddings(json_file_path, torch_path, schema_ex_path, model, device, DATA_KEY, EMBE_KEY)\n",
    "else:\n",
    "    print(f\"JSON path does not exist: {json_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578a3545",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" CHECK EMBEDDDING CONTRUCTION \"\"\"\n",
    "\n",
    "def print_json(pt_path: str) -> None:\n",
    "    try:\n",
    "        if not os.path.exists(pt_path):\n",
    "            print(f\"File không tồn tại: {pt_path}\")\n",
    "            return\n",
    "\n",
    "        data = torch.load(pt_path, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "        if isinstance(data, dict) and f\"{DATA_KEY}\" in data:\n",
    "            content = data[f\"{DATA_KEY}\"]\n",
    "        else:\n",
    "            print(\"Dữ liệu không đúng định dạng: không tìm thấy key 'content'\")\n",
    "            return\n",
    "\n",
    "        if not isinstance(content, list) or not content:\n",
    "            print(\"Dữ liệu rỗng hoặc không phải danh sách\")\n",
    "            return\n",
    "\n",
    "        first_json = content[0]\n",
    "\n",
    "        def process_json(obj: any) -> any:\n",
    "            if isinstance(obj, dict):\n",
    "                return {k: process_json(v) for k, v in obj.items()}\n",
    "            elif isinstance(obj, list) and all(isinstance(x, (float, int)) for x in obj):\n",
    "                return len(obj)\n",
    "            elif isinstance(obj, list):\n",
    "                return [process_json(item) for item in obj]\n",
    "            return obj\n",
    "\n",
    "        processed_json = process_json(first_json)\n",
    "\n",
    "        print(json.dumps(processed_json, ensure_ascii=False, indent=2))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file .pt: {str(e)}\")\n",
    "\n",
    "print_json(torch_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
