{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307a6271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RAGLibrary import myWidgets, myRAG, checkConstruct, createSchema, faissConvert, embedding\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import faiss\n",
    "import pickle\n",
    "import logging\n",
    "import logging\n",
    "import numpy as np\n",
    "from typing import Any, Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64c1389",
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets_list = myWidgets.create_name_form()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1f2401",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "force_download = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426e75ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" DEFINE \"\"\"\n",
    "\n",
    "data   = widgets_list[0] #HBox 1\n",
    "keys   = widgets_list[1] #HBox 2\n",
    "choose = widgets_list[2] #HBox 3\n",
    "\n",
    "embedd_model = widgets_list[3]\n",
    "search_egine = widgets_list[4]\n",
    "rerank_model = widgets_list[5]\n",
    "respon_model = widgets_list[6]\n",
    "API_drop     = widgets_list[7]\n",
    "button_box   = widgets_list[8]\n",
    "\n",
    "# HBox 1\n",
    "file_name = data.children[0]\n",
    "file_type = data.children[1]\n",
    "\n",
    "# HBox 2\n",
    "data_key = keys.children[0]\n",
    "embe_key = keys.children[1]\n",
    "\n",
    "# HBox 3\n",
    "switch_model = choose.children[0]\n",
    "merge_otp    = choose.children[1]\n",
    "path_end_val = choose.children[1]\n",
    "\n",
    "# Get value\n",
    "data_folder   = file_name.value\n",
    "file_type_val = file_type.value\n",
    "\n",
    "data_key_val  = data_key.value\n",
    "embe_key_val  = embe_key.value\n",
    "\n",
    "API_key_val = API_drop.value\n",
    "switch      = switch_model.value\n",
    "merge       = merge_otp.value\n",
    "path_end    = path_end_val.value\n",
    "\n",
    "embedding_model = embedd_model.value\n",
    "searching_egine = search_egine.value\n",
    "reranking_model = rerank_model.value\n",
    "responing_model = respon_model.value\n",
    "\n",
    "\n",
    "# Define\n",
    "base_path = f\"../Data/{data_folder}/{file_type_val}_{data_folder}\"\n",
    "\n",
    "json_file_path = f\"{base_path}_Database.json\"\n",
    "schema_ex_path = f\"{base_path}_Schema.json\"\n",
    "embedding_path = f\"{base_path}_Embeds_{merge}\"\n",
    "\n",
    "torch_path  = f\"{embedding_path}.pt\"\n",
    "faiss_path  = f\"{embedding_path}.faiss\"\n",
    "mapping_path = f\"{embedding_path}_mapping.json\"\n",
    "mapping_data = f\"{embedding_path}_map_data.json\"\n",
    "\n",
    "FILE_TYPE    = file_type_val\n",
    "DATA_KEY     = data_key_val\n",
    "EMBE_KEY     = embe_key_val\n",
    "SWITCH       = switch\n",
    "EMBEDD_MODEL = embedding_model\n",
    "SEARCH_EGINE = searching_egine\n",
    "RERANK_MODEL = reranking_model\n",
    "RESPON_MODEL = responing_model\n",
    "\n",
    "if FILE_TYPE == \"Data\":\n",
    "    MERGE = merge\n",
    "else: \n",
    "    MERGE = \"no_Merge\"\n",
    "\n",
    "API_KEY = API_key_val\n",
    "\n",
    "SEARCH_ENGINE = faiss.IndexFlatIP\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"Embedder: {EMBEDD_MODEL}\")\n",
    "print(f\"Searcher: {SEARCH_EGINE}\")\n",
    "print(f\"Reranker: {RERANK_MODEL}\")\n",
    "print(f\"Responer: {RESPON_MODEL}\")\n",
    "print(f\"Data Key: {DATA_KEY}\")\n",
    "print(f\"Embe Key: {EMBE_KEY}\")\n",
    "print(f\"Database: {json_file_path}\")\n",
    "print(f\"Torch   : {torch_path}\")\n",
    "print(f\"Faiss   : {faiss_path}\")\n",
    "print(f\"Mapping : {mapping_path}\")\n",
    "print(f\"Map Data: {mapping_data}\")\n",
    "print(f\"Schema  : {schema_ex_path}\")\n",
    "print(f\"Model   : {SWITCH}\")\n",
    "print(f\"Merge   : {MERGE}\")\n",
    "print(f\"API Key : {API_KEY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578a3545",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" CHECK EMBEDDDING CONTRUCTION \"\"\"\n",
    "\n",
    "def print_json(pt_path: str) -> None:\n",
    "    try:\n",
    "        if not os.path.exists(pt_path):\n",
    "            print(f\"File không tồn tại: {pt_path}\")\n",
    "            return\n",
    "\n",
    "        data = torch.load(pt_path, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "        if isinstance(data, dict) and f\"{DATA_KEY}\" in data:\n",
    "            content = data[f\"{DATA_KEY}\"]\n",
    "        else:\n",
    "            print(f\"Dữ liệu không đúng định dạng: không tìm thấy key {DATA_KEY}\")\n",
    "            return\n",
    "\n",
    "        if not isinstance(content, list) or not content:\n",
    "            print(\"Dữ liệu rỗng hoặc không phải danh sách\")\n",
    "            return\n",
    "\n",
    "        first_json = content[0]\n",
    "\n",
    "        def process_json(obj: any) -> any:\n",
    "            if isinstance(obj, dict):\n",
    "                return {k: process_json(v) for k, v in obj.items()}\n",
    "            elif isinstance(obj, list) and all(isinstance(x, (float, int)) for x in obj):\n",
    "                return len(obj)\n",
    "            elif isinstance(obj, list):\n",
    "                return [process_json(item) for item in obj]\n",
    "            return obj\n",
    "\n",
    "        processed_json = process_json(first_json)\n",
    "\n",
    "        print(json.dumps(processed_json, ensure_ascii=False, indent=2))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file .pt: {str(e)}\")\n",
    "\n",
    "print_json(torch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5975e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thiết lập logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def inspect_torch_path(torch_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Kiểm tra nội dung file .pt để xác định cấu trúc và dữ liệu.\n",
    "    \n",
    "    Args:\n",
    "        torch_path: Đường dẫn đến file .pt (torch_path từ DEFINE)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Đang tải file .pt: {torch_path}\")\n",
    "        data = torch.load(torch_path, map_location=torch.device('cpu'), weights_only=False)\n",
    "        \n",
    "        logging.info(f\"Kiểu dữ liệu: {type(data)}\")\n",
    "        if isinstance(data, dict):\n",
    "            logging.info(f\"Số lượng khóa cấp cao nhất: {len(data)}\")\n",
    "            for i, (key, value) in enumerate(data.items()):\n",
    "                logging.info(f\"Khóa: {key}, Kiểu giá trị: {type(value)}, Giá trị mẫu: {str(value)[:100]}...\")\n",
    "                if i >= 5:\n",
    "                    break\n",
    "        elif isinstance(data, list):\n",
    "            logging.info(f\"Số lượng phần tử: {len(data)}\")\n",
    "            for i, value in enumerate(data[:5]):\n",
    "                logging.info(f\"Phần tử {i}, Kiểu giá trị: {type(value)}, Giá trị mẫu: {str(value)[:100]}...\")\n",
    "        else:\n",
    "            logging.info(f\"Dữ liệu: {str(data)[:100]}...\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Lỗi khi tải file .pt: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e7c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_and_data(data: Any, prefix: str = \"\") -> Tuple[List[Tuple[str, np.ndarray]], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Trích xuất đệ quy embedding và dữ liệu thông thường từ dữ liệu đầu vào.\n",
    "    Tìm embedding dựa trên khóa chứa 'embedding' (như contents.<i>.Merged_embedding).\n",
    "    \n",
    "    Args:\n",
    "        data: Dữ liệu đầu vào (từ điển, danh sách, v.v.)\n",
    "        prefix: Tiền tố cho khóa\n",
    "    \"\"\"\n",
    "    embeddings_list = []\n",
    "    data_mapping = {}\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            full_key = f\"{prefix}.{key}\" if prefix else key\n",
    "            if isinstance(value, dict):\n",
    "                sub_embeds, sub_data = extract_embeddings_and_data(value, full_key)\n",
    "                embeddings_list.extend(sub_embeds)\n",
    "                data_mapping.update(sub_data)\n",
    "            elif isinstance(value, list) and value and isinstance(value[0], dict):\n",
    "                for i, item in enumerate(value):\n",
    "                    sub_embeds, sub_data = extract_embeddings_and_data(item, f\"{full_key}.{i}\")\n",
    "                    embeddings_list.extend(sub_embeds)\n",
    "                    data_mapping.update(sub_data)\n",
    "            elif isinstance(value, (torch.Tensor, np.ndarray)):\n",
    "                try:\n",
    "                    embedding = value.cpu().numpy() if isinstance(value, torch.Tensor) else value\n",
    "                    if embedding.ndim > 1:\n",
    "                        embedding = embedding.flatten()\n",
    "                    embeddings_list.append((full_key, embedding))\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Lỗi khi xử lý embedding tại {full_key}: {str(e)}\")\n",
    "                    data_mapping[full_key] = value\n",
    "            elif isinstance(value, (list, tuple)) and full_key.lower().find(\"embedding\") != -1:\n",
    "                try:\n",
    "                    embedding = np.array(value, dtype=np.float32)\n",
    "                    if embedding.ndim > 1:\n",
    "                        embedding = embedding.flatten()\n",
    "                    embeddings_list.append((full_key, embedding))\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Lỗi khi chuyển danh sách thành embedding tại {full_key}: {str(e)}\")\n",
    "                    data_mapping[full_key] = value\n",
    "            else:\n",
    "                data_mapping[full_key] = value\n",
    "    \n",
    "    elif isinstance(data, list):\n",
    "        for i, item in enumerate(data):\n",
    "            full_key = f\"{prefix}.item{i}\" if prefix else f\"item{i}\"\n",
    "            if isinstance(item, (dict, list)):\n",
    "                sub_embeds, sub_data = extract_embeddings_and_data(item, full_key)\n",
    "                embeddings_list.extend(sub_embeds)\n",
    "                data_mapping.update(sub_data)\n",
    "            elif isinstance(item, (torch.Tensor, np.ndarray)):\n",
    "                try:\n",
    "                    embedding = item.cpu().numpy() if isinstance(item, torch.Tensor) else item\n",
    "                    if embedding.ndim > 1:\n",
    "                        embedding = embedding.flatten()\n",
    "                    embeddings_list.append((full_key, embedding))\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Lỗi khi xử lý embedding tại {full_key}: {str(e)}\")\n",
    "                    data_mapping[full_key] = item\n",
    "            elif isinstance(item, (list, tuple)) and \"embedding\" in prefix.lower():\n",
    "                try:\n",
    "                    embedding = np.array(item, dtype=np.float32)\n",
    "                    if embedding.ndim > 1:\n",
    "                        embedding = embedding.flatten()\n",
    "                    embeddings_list.append((full_key, embedding))\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Lỗi khi chuyển danh sách thành embedding tại {full_key}: {str(e)}\")\n",
    "                    data_mapping[full_key] = item\n",
    "            else:\n",
    "                data_mapping[full_key] = item\n",
    "    \n",
    "    return embeddings_list, data_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d10bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index(embeddings: List[Tuple[str, np.ndarray]], nlist: int = 100) -> Tuple[faiss.Index, Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Tạo chỉ mục FAISS (IndexFlatIP) từ danh sách (khóa, embedding).\n",
    "    \n",
    "    Args:\n",
    "        embeddings: Danh sách các cặp (khóa, embedding)\n",
    "        nlist: Số lượng cụm cho IndexFlatIP\n",
    "    \"\"\"\n",
    "    if not embeddings:\n",
    "        raise ValueError(\"Không tìm thấy embedding trong dữ liệu đầu vào. Vui lòng kiểm tra file .pt.\")\n",
    "    \n",
    "    embedding_dim = len(embeddings[0][1])\n",
    "    if not all(len(emb) == embedding_dim for _, emb in embeddings):\n",
    "        raise ValueError(\"Tất cả embedding phải có cùng chiều.\")\n",
    "    \n",
    "    embedding_matrix = np.array([emb for _, emb in embeddings]).astype('float32')    \n",
    "    logging.info(\"Đang thêm embedding vào chỉ mục...\")\n",
    "\n",
    "    embedding_dim = embedding_matrix.shape[1]\n",
    "    \n",
    "    # Tạo IndexFlatIP\n",
    "    index = faiss.IndexFlatIP(embedding_dim)\n",
    "    index.add(embedding_matrix)\n",
    "    \n",
    "    key_to_index = {key: idx for idx, (key, _) in enumerate(embeddings)}\n",
    "    \n",
    "    return index, key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7f2a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pt_to_faiss(torch_path: str, faiss_path: str, mapping_path: str, mapping_data: str, data_key: str, nlist: int = 100, use_pickle: bool = False) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Chuyển file .pt sang chỉ mục FAISS và lưu ánh xạ khóa cùng dữ liệu thông thường.\n",
    "    Sử dụng torch_path (torch_path), faiss_path, mapping_path, mapping_data từ DEFINE.\n",
    "    \n",
    "    Args:\n",
    "        torch_path: Đường dẫn đến file .pt (torch_path)\n",
    "        faiss_path: Đường dẫn lưu chỉ mục FAISS\n",
    "        mapping_path: Đường dẫn lưu ánh xạ khóa\n",
    "        mapping_data: Đường dẫn lưu dữ liệu thông thường\n",
    "        use_pickle: Nếu True, lưu dưới dạng pickle thay vì JSON\n",
    "        nlist: Số lượng cụm cho IndexFlatIP\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Kiểm tra file .pt tồn tại\n",
    "        if not os.path.exists(torch_path):\n",
    "            raise FileNotFoundError(f\"File .pt không tồn tại: {torch_path}\")\n",
    "        \n",
    "        # Tạo thư mục đầu ra nếu chưa tồn tại\n",
    "        os.makedirs(os.path.dirname(faiss_path), exist_ok=True)\n",
    "        \n",
    "        # Kiểm tra cấu trúc file .pt\n",
    "        inspect_torch_path(torch_path)\n",
    "        \n",
    "        # Tải file .pt\n",
    "        logging.info(f\"Đang tải file .pt: {torch_path}\")\n",
    "        data = torch.load(torch_path, map_location=torch.device('cpu'), weights_only=False)\n",
    "        \n",
    "        # Trích xuất embedding và dữ liệu thông thường\n",
    "        logging.info(\"Đang trích xuất embedding và dữ liệu...\")\n",
    "        embeddings_list, data_mapping = extract_embeddings_and_data(data)\n",
    "        \n",
    "        # Kiểm tra xem có embedding nào không\n",
    "        if not embeddings_list:\n",
    "            logging.error(\"Không tìm thấy embedding nào trong file .pt. Vui lòng kiểm tra cấu trúc dữ liệu.\")\n",
    "            raise ValueError(\"Không tìm thấy embedding nào trong file .pt.\")\n",
    "        \n",
    "        logging.info(f\"Tìm thấy {len(embeddings_list)} embedding.\")\n",
    "        \n",
    "        # Tạo chỉ mục FAISS\n",
    "        logging.info(\"Đang tạo chỉ mục FAISS...\")\n",
    "        faiss_index, key_to_index = create_faiss_index(embeddings_list, nlist=nlist)\n",
    "        \n",
    "        # Lưu chỉ mục FAISS\n",
    "        logging.info(f\"Đang lưu chỉ mục FAISS vào {faiss_path}\")\n",
    "        faiss.write_index(faiss_index, faiss_path)\n",
    "        \n",
    "        # Lưu ánh xạ khóa sang chỉ số\n",
    "        logging.info(f\"Đang lưu ánh xạ khóa vào {mapping_path}\")\n",
    "        if use_pickle:\n",
    "            with open(mapping_path, 'wb') as f:\n",
    "                pickle.dump(key_to_index, f)\n",
    "        else:\n",
    "            with open(mapping_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(key_to_index, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "        # Lưu dữ liệu thông thường\n",
    "        logging.info(f\"Đang lưu dữ liệu thông thường vào {mapping_data}\")\n",
    "        if use_pickle:\n",
    "            with open(mapping_data, 'wb') as f:\n",
    "                pickle.dump(data_mapping, f)\n",
    "        else:\n",
    "            with open(mapping_data, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data_mapping, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "        logging.info(\"Chuyển đổi hoàn tất.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Lỗi trong quá trình chuyển đổi: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb83e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\" MAIN - CONVERT TO FAISS \"\"\"\"\"\n",
    "if os.path.exists(faiss_path):\n",
    "    print(f\"\\nFaiss loaded from {faiss_path}\\n\")\n",
    "else:\n",
    "    if os.path.exists(torch_path):\n",
    "        convert_pt_to_faiss(torch_path=torch_path, faiss_path=faiss_path, mapping_path=mapping_path, mapping_data=mapping_data, data_key = DATA_KEY, nlist = 100, use_pickle = False)\n",
    "    else:\n",
    "        print(f\"TORCH path does not exist: {torch_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
