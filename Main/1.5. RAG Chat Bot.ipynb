{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RAGLibrary import myWidgets, myRAG, checkConstruct, createSchema, faissConvert, embedding\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import faiss\n",
    "import pickle\n",
    "import logging\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from typing import Any, Dict, List\n",
    "import google.generativeai as genai\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from google.api_core.exceptions import ResourceExhausted\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets_list = myWidgets.create_name_form()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "force_download = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" DEFINE \"\"\"\n",
    "\n",
    "data   = widgets_list[0] #HBox 1\n",
    "keys   = widgets_list[1] #HBox 2\n",
    "choose = widgets_list[2] #HBox 3\n",
    "\n",
    "embedd_model = widgets_list[3]\n",
    "search_egine = widgets_list[4]\n",
    "rerank_model = widgets_list[5]\n",
    "respon_model = widgets_list[6]\n",
    "API_drop     = widgets_list[7]\n",
    "button_box   = widgets_list[8]\n",
    "\n",
    "# HBox 1\n",
    "file_name = data.children[0]\n",
    "file_type = data.children[1]\n",
    "\n",
    "# HBox 2\n",
    "data_key = keys.children[0]\n",
    "embe_key = keys.children[1]\n",
    "\n",
    "# HBox 3\n",
    "switch_model = choose.children[0]\n",
    "merge_otp    = choose.children[1]\n",
    "path_end_val = choose.children[1]\n",
    "\n",
    "# Get value\n",
    "data_folder   = file_name.value\n",
    "file_type_val = file_type.value\n",
    "\n",
    "data_key_val  = data_key.value\n",
    "embe_key_val  = embe_key.value\n",
    "\n",
    "API_key_val = API_drop.value\n",
    "switch      = switch_model.value\n",
    "merge       = merge_otp.value\n",
    "path_end    = path_end_val.value\n",
    "\n",
    "embedding_model = embedd_model.value\n",
    "searching_egine = search_egine.value\n",
    "reranking_model = rerank_model.value\n",
    "responing_model = respon_model.value\n",
    "\n",
    "\n",
    "# Define\n",
    "base_path = f\"../Data/{data_folder}/{file_type_val}_{data_folder}\"\n",
    "\n",
    "json_file_path = f\"{base_path}_Database.json\"\n",
    "schema_ex_path = f\"{base_path}_Schema.json\"\n",
    "embedding_path = f\"{base_path}_Embeds_{merge}\"\n",
    "\n",
    "torch_path  = f\"{embedding_path}.pt\"\n",
    "faiss_path  = f\"{embedding_path}.faiss\"\n",
    "mapping_path = f\"{embedding_path}_mapping.json\"\n",
    "mapping_data = f\"{embedding_path}_map_data.json\"\n",
    "\n",
    "FILE_TYPE    = file_type_val\n",
    "DATA_KEY     = data_key_val\n",
    "EMBE_KEY     = embe_key_val\n",
    "SWITCH       = switch\n",
    "EMBEDD_MODEL = embedding_model\n",
    "SEARCH_EGINE = searching_egine\n",
    "RERANK_MODEL = reranking_model\n",
    "RESPON_MODEL = responing_model\n",
    "\n",
    "if FILE_TYPE == \"Data\":\n",
    "    MERGE = merge\n",
    "else: \n",
    "    MERGE = \"no_Merge\"\n",
    "\n",
    "API_KEY = API_key_val\n",
    "\n",
    "SEARCH_ENGINE = faiss.IndexFlatIP\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"Embedder: {EMBEDD_MODEL}\")\n",
    "print(f\"Searcher: {SEARCH_EGINE}\")\n",
    "print(f\"Reranker: {RERANK_MODEL}\")\n",
    "print(f\"Responer: {RESPON_MODEL}\")\n",
    "print(f\"Data Key: {DATA_KEY}\")\n",
    "print(f\"Embe Key: {EMBE_KEY}\")\n",
    "print(f\"Database: {json_file_path}\")\n",
    "print(f\"Torch   : {torch_path}\")\n",
    "print(f\"Faiss   : {faiss_path}\")\n",
    "print(f\"Mapping : {mapping_path}\")\n",
    "print(f\"Map Data: {mapping_data}\")\n",
    "print(f\"Schema  : {schema_ex_path}\")\n",
    "print(f\"Model   : {SWITCH}\")\n",
    "print(f\"Merge   : {MERGE}\")\n",
    "print(f\"API Key : {API_KEY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if (SWITCH == \"Auto Model\"):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(EMBEDD_MODEL, force_download=force_download)\n",
    "        model = AutoModel.from_pretrained(EMBEDD_MODEL, force_download=force_download)\n",
    "        model = model.to(device)\n",
    "        print(\"Model and tokenizer loaded successfully\")\n",
    "    except Exception as e:\n",
    "        raise\n",
    "elif (SWITCH == \"Sentence Transformer\"):\n",
    "    try:\n",
    "        # model = SentenceTransformer(EMBEDD_MODEL).to(device)\n",
    "        model = SentenceTransformer(\"../../cached_model\")\n",
    "        print(\"SentenceTransformer loaded successfully\")\n",
    "    except Exception as e:\n",
    "        raise\n",
    "\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PREPROCESS TEXT \"\"\"\n",
    "\n",
    "def preprocess_text(text):\n",
    "    import re\n",
    "    if isinstance(text, list):\n",
    "        return [preprocess_text(t) for t in text]\n",
    "    if isinstance(text, str):\n",
    "        text = text.strip()\n",
    "        text = re.sub(r'[^\\w\\s\\(\\)\\.\\,\\;\\:\\-–]', '', text)\n",
    "        text = re.sub(r'[ ]{2,}', ' ', text)\n",
    "        return text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" CREATE EMBEDDING \"\"\"\n",
    "\n",
    "def create_embedding(texts, batch_size=32):\n",
    "    try:\n",
    "        embeddings = model.encode(texts, batch_size=batch_size, convert_to_tensor=True, device=device)\n",
    "        return embeddings\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(\"VRAM overflow. Switching to CPU.\")\n",
    "            model.to(\"cpu\")\n",
    "            return model.encode(texts, batch_size=batch_size, convert_to_tensor=True, device=\"cpu\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\" SEARCH \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Tìm kiếm văn bản liên quan đến câu hỏi sử dụng FAISS IndexFlatIP.\n",
    "\n",
    "Args:\n",
    "    query: Câu hỏi dạng văn bản\n",
    "    embedd_model: Tên mô hình embedding (EMBEDD_MODEL từ DEFINE)\n",
    "    search_engine: Loại chỉ mục FAISS (SEARCH_ENGINE từ DEFINE)\n",
    "    faiss_path: Đường dẫn chỉ mục FAISS (faiss_path từ DEFINE)\n",
    "    mapping_path: Đường dẫn file ánh xạ (mapping_path từ DEFINE)\n",
    "    data_path: Đường dẫn file dữ liệu text (mapping_data từ DEFINE)\n",
    "    data_key: Khóa dữ liệu (DATA_KEY, ví dụ: contents)\n",
    "    device: Thiết bị PyTorch (device từ DEFINE, ví dụ: cuda hoặc cpu)\n",
    "    k: Số lượng kết quả trả về\n",
    "\n",
    "Returns:\n",
    "    Danh sách các kết quả: {\"text\": văn bản, \"faiss_score\": điểm FAISS}\n",
    "\"\"\"\n",
    "\n",
    "def search_faiss_index(\n",
    "    query: str,\n",
    "    embedd_model: str,\n",
    "    faiss_path: str,\n",
    "    mapping_path: str,\n",
    "    data_path: str,\n",
    "    device: str = \"cuda\",\n",
    "    k: int = 10\n",
    ") -> List[Dict[str, Any]]:\n",
    "\n",
    "    try:\n",
    "        # model = SentenceTransformer(embedd_model, device=device)\n",
    "        query_embedding = model.encode(query, convert_to_tensor=True, device=device).cpu().numpy()\n",
    "        \n",
    "        # Chuẩn hóa query embedding cho IndexFlatIP\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=-1, keepdims=True)\n",
    "        \n",
    "        index = faiss.read_index(faiss_path)\n",
    "                \n",
    "        with open(mapping_path, 'r', encoding='utf-8') as f:\n",
    "            key_to_index = json.load(f)\n",
    "\n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            data_mapping = json.load(f)\n",
    "        \n",
    "        # Tìm kiếm k kết quả gần nhất\n",
    "        scores, indices = index.search(query_embedding.reshape(1, -1), k)\n",
    "        \n",
    "        # Ánh xạ\n",
    "        results = []\n",
    "        index_to_key = {v: k for k, v in key_to_index.items()}\n",
    "        for idx, score in zip(indices[0], scores[0]):\n",
    "            if idx not in index_to_key:\n",
    "                continue\n",
    "            key = index_to_key[idx]\n",
    "            text_key = key.replace(\"Merged_embedding\", \"Merged_text\")\n",
    "            text = data_mapping.get(text_key, \"\")\n",
    "            if not text:\n",
    "                text = next((v for k, v in data_mapping.items() if k.startswith(key.split(\"Merged_embedding\")[0]) and isinstance(v, (str, list))), \"\")\n",
    "            text = text if isinstance(text, str) else \" \".join(text) if isinstance(text, list) else \"\"\n",
    "            if text:\n",
    "                results.append({\n",
    "                    \"text\": text,\n",
    "                    \"faiss_score\": float(score),\n",
    "                    \"key\": key\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during search: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" RERANK \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Xếp hạng lại kết quả sử dụng mô hình reranker.\n",
    "\n",
    "Args:\n",
    "    query: Câu hỏi dạng văn bản\n",
    "    results: Danh sách kết quả sơ bộ từ search_faiss_index\n",
    "    reranker_model: Tên mô hình reranker (RERANK_MODEL từ DEFINE)\n",
    "    device: Thiết bị PyTorch (cuda hoặc cpu)\n",
    "    k: Số lượng kết quả trả về sau reranking\n",
    "\n",
    "Returns:\n",
    "    Danh sách các kết quả: {\"text\": văn bản, \"rerank_score\": điểm reranker, \"faiss_score\": điểm FAISS}\n",
    "\"\"\"\n",
    "\n",
    "def rerank_results(\n",
    "    query: str,\n",
    "    results: List[Dict[str, Any]],\n",
    "    reranker_model: str,\n",
    "    device: str = \"cuda\",\n",
    "    k: int = 5\n",
    ") -> List[Dict[str, Any]]:\n",
    "    try:\n",
    "        if not results:\n",
    "            return []\n",
    "        \n",
    "        # Tải mô hình reranker\n",
    "        reranker = CrossEncoder(reranker_model, device=device)\n",
    "        \n",
    "        # Tạo cặp [query, text] để rerank\n",
    "        pairs = [[query, result[\"text\"]] for result in results]\n",
    "        \n",
    "        # Tính điểm rerank\n",
    "        rerank_scores = reranker.predict(pairs)\n",
    "        \n",
    "        # Gắn điểm rerank vào kết quả\n",
    "        for i, score in enumerate(rerank_scores):\n",
    "            results[i][\"rerank_score\"] = float(score)\n",
    "        \n",
    "        # Sắp xếp theo rerank_score và lấy top k\n",
    "        sorted_results = sorted(results, key=lambda x: x[\"rerank_score\"], reverse=True)[:k]\n",
    "        \n",
    "        # Định dạng kết quả cuối\n",
    "        final_results = [\n",
    "            {\n",
    "                \"text\": result[\"text\"],\n",
    "                \"rerank_score\": result[\"rerank_score\"],\n",
    "                \"faiss_score\": result[\"faiss_score\"]\n",
    "            }\n",
    "            for result in sorted_results\n",
    "        ]\n",
    "        \n",
    "        return final_results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during rerank: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" RESPOND \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Lọc kết quả rerank và sinh câu trả lời tự nhiên bằng Gemini 1.5 Pro.\n",
    "\n",
    "Args:\n",
    "    query: Câu hỏi dạng văn bản\n",
    "    results: Danh sách kết quả từ rerank_results ({'text', 'rerank_score', 'faiss_score', 'key'})\n",
    "    responser_model: Tên mô hình Gemini (mặc định gemini-1.5-pro)\n",
    "    device: Thiết bị PyTorch (cuda hoặc cpu, chỉ để tương thích)\n",
    "    score_threshold: Ngưỡng rerank_score để lọc\n",
    "    max_results: Số kết quả tối đa để tổng hợp\n",
    "    gemini_api_key: API key của Google AI Studio\n",
    "\n",
    "Returns:\n",
    "    Tuple: (câu trả lời tự nhiên, danh sách kết quả được lọc)\n",
    "\"\"\"\n",
    "\n",
    "def respond_naturally(\n",
    "    query: str,\n",
    "    results: List[Dict[str, Any]],\n",
    "    responser_model: str = \"gemini-2.0-flash-exp\",\n",
    "    score_threshold: float = 0.85,\n",
    "    max_results: int = 3,\n",
    "    gemini_api_key: str = None\n",
    ") -> tuple[str, List[Dict[str, Any]]]:\n",
    "\n",
    "    try:\n",
    "        # Lọc kết quả theo ngưỡng rerank_score và độ dài văn bản\n",
    "        filtered_results = [\n",
    "            r for r in results\n",
    "            if r[\"rerank_score\"] > score_threshold and len(r[\"text\"]) > 50\n",
    "        ][:max_results]\n",
    "        \n",
    "        if not filtered_results:\n",
    "            return \"Không tìm thấy thông tin phù hợp với câu hỏi.\", []\n",
    "        \n",
    "        # Ghép văn bản được lọc thành context\n",
    "        context = \"\\n\".join([r[\"text\"] for r in filtered_results])\n",
    "        \n",
    "        genai.configure(api_key=gemini_api_key)\n",
    "        \n",
    "        # Kiểm tra trạng thái mô hình\n",
    "        model = genai.GenerativeModel(responser_model)\n",
    "        \n",
    "        # Tạo prompt cho mô hình\n",
    "        prompt = (\n",
    "            f\"Câu hỏi: {query}\\n\"\n",
    "            f\"Thông tin: {context}\\n\"\n",
    "            f\"Trả lời ngắn gọn và tự nhiên bằng tiếng Việt:\"\n",
    "        )\n",
    "        \n",
    "        # Sinh câu trả lời\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config={\n",
    "                \"max_output_tokens\": 200,\n",
    "                \"temperature\": 0.7,\n",
    "                \"top_p\": 0.9\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Xử lý response\n",
    "        if hasattr(response, \"candidates\") and response.candidates:\n",
    "            candidate = response.candidates[0]\n",
    "            if hasattr(candidate, \"content\") and candidate.content.parts:\n",
    "                response_text = candidate.content.parts[0].text.strip()\n",
    "            else:\n",
    "                raise ValueError(\"Không tìm thấy nội dung trong candidate của Gemini API.\")\n",
    "        else:\n",
    "            raise ValueError(\"Response không có candidates.\")\n",
    "\n",
    "        return response_text, filtered_results\n",
    "    \n",
    "    except ResourceExhausted as e:\n",
    "        error_msg = f\"Vượt giới hạn API\"\n",
    "        print(error_msg)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" MAIN \"\"\"\n",
    "\n",
    "print(\"<< Enter 'exit', 'quit', 'escape', 'bye' or Press ESC to exit >>\")\n",
    "print(\"Chatbot: Hello there! I'm here to help you =))\")\n",
    "\n",
    "user_input = \"Quy định về đào tạo đại học tại trường Thủ đô Hà Nội\"\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # user_input = input(\"You: \")\n",
    "        user_question = preprocess_text(user_input)\n",
    "        print(f\"You: {user_question}\")\n",
    "        os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "        if user_input.strip().lower() in [\"exit\", \"quit\", \"escape\", \"bye\", \"\"]:\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        #Bước 1: Search\n",
    "        preliminary_results = search_faiss_index(\n",
    "            query= user_question,\n",
    "            embedd_model=EMBEDD_MODEL,\n",
    "            faiss_path=faiss_path,\n",
    "            mapping_path=mapping_path,\n",
    "            data_path=mapping_data,\n",
    "            device=device,\n",
    "            k=10\n",
    "        )\n",
    "        \n",
    "        # Bước 2: Rerank\n",
    "        reranked_results = rerank_results(\n",
    "            query= user_question,\n",
    "            results=preliminary_results,\n",
    "            reranker_model=RERANK_MODEL,\n",
    "            device=device,\n",
    "            k=5,\n",
    "        )\n",
    "\n",
    "        # Bước 3: Generate Response\n",
    "        response, filtered_results = respond_naturally(\n",
    "            query= user_question,\n",
    "            results=reranked_results,\n",
    "            responser_model=RESPON_MODEL,\n",
    "            score_threshold=0.85,\n",
    "            max_results=3,\n",
    "            gemini_api_key=API_KEY\n",
    "        )\n",
    "\n",
    "        print(\"Câu trả lời:\")\n",
    "        print(response)\n",
    "        user_input = \"exit\"\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nChatbot: Goodbye!\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
