{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import fitz\n",
        "import json\n",
        "import spacy\n",
        "import pickle\n",
        "import win32com.client\n",
        "from docx import Document\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Javascript\n",
        "\n",
        "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "exceptions = {\n",
        "    \"a\", \"an\", \"the\",\n",
        "    \"and\", \"but\", \"or\", \"nor\", \"for\", \"so\", \"yet\",\n",
        "    \"at\", \"by\", \"in\", \"of\", \"on\", \"to\", \"from\", \"with\", \"as\",\n",
        "    \"into\", \"like\", \"over\", \"under\", \"up\", \"down\", \"out\", \"upon\", \"onto\",  \n",
        "    \"amid\", \"among\", \"between\", \"before\", \"after\", \"against\"  \n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "state_file = \"#Chunks_Size.pkl\"\n",
        "\n",
        "def save_state():\n",
        "    state = {\n",
        "        \"input_drop\": doc_drop.value,\n",
        "        \"word_input\": word_input.value,\n",
        "        \"level_input\": level_input.value,\n",
        "        \"level_values\": {i: text.value for i, text in enumerate(input_box.children)},\n",
        "    }\n",
        "    with open(state_file, \"wb\") as f:\n",
        "        pickle.dump(state, f)\n",
        "\n",
        "def load_state():\n",
        "    if os.path.exists(state_file):\n",
        "        with open(state_file, \"rb\") as f:\n",
        "            return pickle.load(f)\n",
        "    return {}\n",
        "\n",
        "input_folder_path = \"../Doc\"\n",
        "\n",
        "input_folder = os.listdir(input_folder_path)\n",
        "\n",
        "state = load_state()\n",
        "\n",
        "doc_drop = widgets.Dropdown(\n",
        "    options=input_folder,\n",
        "    description=\"Input File:  \",\n",
        "    disabled=False,\n",
        "    layout=widgets.Layout(width=\"50%\"),\n",
        "    value=state.get(\"input_drop\", input_folder[0] if input_folder else None),\n",
        ")\n",
        "\n",
        "word_input = widgets.Text(\n",
        "    description=\"Word Limit: \",\n",
        "    placeholder=\"Default: 200\",\n",
        "    layout=widgets.Layout(width=\"50%\"),\n",
        "    value=state.get(\"word_input\", \"\"),\n",
        ")\n",
        "\n",
        "level_input = widgets.Dropdown(\n",
        "    description=\"Max Level: \",\n",
        "    options=[str(i) for i in range(0, 10)],\n",
        "    layout=widgets.Layout(width=\"50%\"),\n",
        "    value=state.get(\"level_input\", \"1\"),\n",
        ")\n",
        "\n",
        "input_box = widgets.VBox([])\n",
        "\n",
        "def update_text_inputs(change):\n",
        "    level_number = int(change.new)\n",
        "    prev_values = state.get(\"level_values\", [])\n",
        "\n",
        "    text_inputs = [\n",
        "        widgets.Text(\n",
        "            description=f\"Level {i+1}: \",\n",
        "            layout=widgets.Layout(width=\"50%\"),\n",
        "            value=prev_values[i] if i < len(prev_values) else \"\"\n",
        "        )\n",
        "        for i in range(level_number)\n",
        "    ]\n",
        "    input_box.children = text_inputs\n",
        "\n",
        "level_input.observe(update_text_inputs, names=\"value\")\n",
        "\n",
        "save_button = widgets.Button(description=\"Save State\", button_style=\"success\")\n",
        "run_button = widgets.Button(description=\"Run All Below\", button_style=\"primary\")\n",
        "\n",
        "def on_save_clicked(b):\n",
        "    save_state()\n",
        "\n",
        "def on_run_clicked(b):\n",
        "    save_state()\n",
        "    display(Javascript(\"Jupyter.notebook.execute_cells_below()\"))\n",
        "\n",
        "save_button.on_click(on_save_clicked)\n",
        "run_button.on_click(on_run_clicked)\n",
        "\n",
        "button_box = widgets.HBox(\n",
        "    [save_button, run_button],\n",
        "    layout=widgets.Layout(\n",
        "        width=\"50%\", \n",
        "        justify_content=\"space-between\", \n",
        "        padding=\"0px 4% 0px 12%\",\n",
        "    )\n",
        ")\n",
        "\n",
        "footer_display = widgets.HBox(\n",
        "    [button_box],\n",
        "    layout=widgets.Layout(\n",
        "        width=\"90%\", \n",
        "        justify_content=\"space-between\", \n",
        "        padding=\"10px 5% 10px 5%\",\n",
        "    )\n",
        ")\n",
        "\n",
        "display(doc_drop, word_input, level_input, input_box, footer_display)\n",
        "\n",
        "level_input.value = \"0\"\n",
        "level_input.value = state.get(\"level_input\", \"0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# levels = {}\n",
        "\n",
        "# for i, text_input in enumerate(input_box.children):\n",
        "#     levels[i+1] = text_input.value\n",
        "#     print(f\"level {i+1}: {levels[i+1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_name = os.path.splitext(doc_drop.value)[0]\n",
        "\n",
        "input_path = f\"../Doc/{doc_drop.value}\"\n",
        "\n",
        "output_folder = f\"../Data/{file_name}\"\n",
        "os.makedirs(f\"{output_folder}\", exist_ok=True)\n",
        "\n",
        "chunks_base = f\"{output_folder}/Data_{file_name}_Base.json\"\n",
        "chunks_final = f\"{output_folder}/Data_{file_name}_Chunk.json\"\n",
        "embedding_file = f\"Embeddings_{output_folder}/{file_name}\"\n",
        "\n",
        "print(input_path)\n",
        "print(output_folder)\n",
        "path = input_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BASE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXTRACT INPUT TEXT\n",
        "def extracted(path):\n",
        "    text_data = []\n",
        "    doc = Document(path)\n",
        "    \n",
        "    for para in doc.paragraphs:\n",
        "        cleaned_text = ' '.join(para.text.strip().split())\n",
        "        if cleaned_text:\n",
        "            font_size = para.runs[0].font.size.pt if para.runs and para.runs[0].font.size else 0\n",
        "            text_data.append({\"text\": cleaned_text, \"font_size\": font_size})\n",
        "    \n",
        "    return text_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FORMAT TEXT\n",
        "def format_text(text, case=\"upper\"):\n",
        "    if case == \"upper\":\n",
        "        return text.upper()\n",
        "    elif case == \"Chapter\":\n",
        "        return ' '.join(word.capitalize() if word.lower() not in exceptions else word.lower() for word in text.split())\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SIZE OF Chapter AND Title\n",
        "def find_sizes(text_data):\n",
        "    Chapter_size, sub_size = 0, 0\n",
        "    \n",
        "    for entry in text_data:\n",
        "        text, font_size = entry[\"text\"], entry[\"font_size\"]\n",
        "        if text.isupper():\n",
        "            Chapter_size = max(Chapter_size, font_size)\n",
        "        else:\n",
        "            sub_size = max(sub_size, font_size)\n",
        "    return Chapter_size, sub_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ADD CHUNKS\n",
        "def add_chunk(chunks, content):\n",
        "    if content[\"Content\"]:\n",
        "        chunks.append(content.copy())\n",
        "        content[\"Content\"] = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MAIN FUNCTION\n",
        "def main(text_data, Chapter_size, sub_size):\n",
        "    chunks = []\n",
        "    content = {\"Chapter\": None, \"Title\": None, \"Article\": None, \"Content\": []}\n",
        "\n",
        "    for entry in text_data:\n",
        "        text, font_size = entry[\"text\"], entry[\"font_size\"]\n",
        "        if not text:\n",
        "            continue\n",
        "        if text.isupper():\n",
        "            if font_size == Chapter_size:\n",
        "                add_chunk(chunks, content)\n",
        "                content[\"Chapter\"] = format_text(text, \"upper\")\n",
        "                content[\"Title\"] = None\n",
        "                content[\"Article\"] = None\n",
        "            else:\n",
        "                add_chunk(chunks, content)\n",
        "                content[\"Article\"] = format_text(text, \"upper\")\n",
        "        else:\n",
        "            if font_size == sub_size:\n",
        "                add_chunk(chunks, content)\n",
        "                content[\"Title\"] = format_text(text, \"Chapter\")\n",
        "                content[\"Article\"] = None\n",
        "            else:\n",
        "                if content[\"Content\"]:\n",
        "                    last_sentence = content[\"Content\"][-1]\n",
        "                    if not last_sentence.endswith((\".\", \"!\", \"?\")):\n",
        "                        content[\"Content\"][-1] += \" \" + text\n",
        "                    else:\n",
        "                        content[\"Content\"].append(text)\n",
        "                else:\n",
        "                    content[\"Content\"].append(text)\n",
        "    \n",
        "    add_chunk(chunks, content)\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CHUNKS BASE\n",
        "text_data = extracted(input_path)\n",
        "Chapter_size, sub_size = find_sizes(text_data)\n",
        "chunks = main(text_data, Chapter_size, sub_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXPORT BASE\n",
        "with open(chunks_base, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(chunks, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"Base data saved to {chunks_base}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "FINAL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "WORD_LIMIT = int(word_input.value) if word_input.value.isdigit() else 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# COUNT WORDS\n",
        "def count_words(text):\n",
        "    return len(text.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CHUNKING IF WORD LIMIT EXCEEDED\n",
        "def semantic_chunking(text, max_words=WORD_LIMIT):\n",
        "\n",
        "    doc = nlp(text)\n",
        "    chunks, current_chunk = [], []\n",
        "    word_count = 0\n",
        "    \n",
        "    for sent in doc.sents:\n",
        "        sentence = sent.text.strip()\n",
        "        sentence_length = len(sentence.split())\n",
        "        \n",
        "        if word_count + sentence_length > max_words and current_chunk:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = []\n",
        "            word_count = 0\n",
        "            \n",
        "        current_chunk.append(sentence)\n",
        "        word_count += sentence_length\n",
        "        \n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "        \n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MAIN PROCESSING FUNCTION\n",
        "def process_json(chunks_base, chunks_final):\n",
        "    with open(chunks_base, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "        \n",
        "    processed_data = []\n",
        "    \n",
        "    for idx, chunk in enumerate(data):\n",
        "        \n",
        "        # PRINT PROGRESS INFO       \n",
        "        if \"Content\" in chunk and isinstance(chunk[\"Content\"], list):\n",
        "            new_content = []\n",
        "            \n",
        "            for para_idx, paragraph in enumerate(chunk[\"Content\"]):\n",
        "                word_count = count_words(paragraph)\n",
        "                \n",
        "                if word_count > WORD_LIMIT:\n",
        "                    \n",
        "                    # PRINT WORDS NUMBER\n",
        "                    print(f\"{idx+1:04} / {len(data):04}: {para_idx+1:02}: {word_count} words.\")\n",
        "                    \n",
        "                    chunked_paragraphs = semantic_chunking(paragraph)\n",
        "                    new_content.extend(chunked_paragraphs)\n",
        "                    \n",
        "                    # PRINT SEGMENTS NUMBER\n",
        "                    print(f\"{idx+1:04} / {len(data):04}: {len(chunked_paragraphs):02} segments.\")\n",
        "                    \n",
        "                else:\n",
        "                    new_content.append(paragraph)\n",
        "                    \n",
        "            chunk[\"Content\"] = new_content\n",
        "            \n",
        "        processed_data.append(chunk)\n",
        "        \n",
        "        # SAVE PROGRESS\n",
        "        with open(chunks_final, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(processed_data, f, indent=4, ensure_ascii=False)\n",
        "                \n",
        "    # FINISHED    \n",
        "    print(f\"Final data saved to {chunks_final}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "process_json(chunks_base, chunks_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "EMBEDDING\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "orias",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
