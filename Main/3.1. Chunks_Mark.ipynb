{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import fitz\n",
        "import spacy\n",
        "import faiss\n",
        "import pickle\n",
        "import win32com.client\n",
        "from docx import Document\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Javascript\n",
        "from RAGLibrary import Widgets\n",
        "\n",
        "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "widgets_list = Widgets.create_name_form()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" DEFINE \"\"\"\n",
        "\n",
        "\"\"\" GET PARENT VALUE \"\"\"\n",
        "data   = widgets_list[0] #HBox 1\n",
        "keys   = widgets_list[1] #HBox 2\n",
        "choose = widgets_list[2] #HBox 3\n",
        "\n",
        "embedd_model = widgets_list[3]\n",
        "search_egine = widgets_list[4]\n",
        "rerank_model = widgets_list[5]\n",
        "respon_model = widgets_list[6]\n",
        "API_drop     = widgets_list[7]\n",
        "\n",
        "# Hbox 4\n",
        "chunk_input = widgets_list[8]\n",
        "\n",
        "# HBox 5\n",
        "level_value = widgets_list[9]\n",
        "\n",
        "# Button\n",
        "button_box  = widgets_list[10]\n",
        "\n",
        "\n",
        "\"\"\" GET CHILDREN VALUE \"\"\"\n",
        "\n",
        "# HBox 1\n",
        "file_name = data.children[0]\n",
        "file_type = data.children[1]\n",
        "path_end  = data.children[2]\n",
        "\n",
        "# HBox 2\n",
        "data_key = keys.children[0]\n",
        "embe_key = keys.children[1]\n",
        "\n",
        "# HBox 3\n",
        "switch_model = choose.children[0]\n",
        "merge_otp    = choose.children[1]\n",
        "\n",
        "# HBox 4\n",
        "level_input = chunk_input.children[0]\n",
        "word_limit  = chunk_input.children[1]\n",
        "\n",
        "# HBox 5\n",
        "level_values = [child.value for child in level_value.children]\n",
        "\n",
        "\"\"\" DEF VALUE \"\"\"\n",
        "\n",
        "# Get value\n",
        "data_folder   = file_name.value\n",
        "file_type_val = file_type.value\n",
        "\n",
        "data_key_val  = data_key.value\n",
        "embe_key_val  = embe_key.value\n",
        "\n",
        "API_key_val = API_drop.value\n",
        "switch      = switch_model.value\n",
        "merge       = merge_otp.value\n",
        "path_end_v  = path_end.value\n",
        "\n",
        "embedding_model = embedd_model.value\n",
        "searching_egine = search_egine.value\n",
        "reranking_model = rerank_model.value\n",
        "responing_model = respon_model.value\n",
        "\n",
        "level_input_val = level_input.value\n",
        "word_limit_val  = word_limit.value\n",
        "\n",
        "\n",
        "\"\"\" DETAIL \"\"\"\n",
        "\n",
        "# Define\n",
        "dcmt_path = f\"../Doc/{data_folder}{path_end_v}\"\n",
        "\n",
        "base_folder = \"../Data\"\n",
        "base_path = f\"{base_folder}/{data_folder}/{file_type_val}_{data_folder}\"\n",
        "\n",
        "chunks_base    = f\"{base_path}_Chunks.json\"\n",
        "json_file_path = f\"{base_path}_Database.json\"\n",
        "schema_ex_path = f\"{base_path}_Schema.json\"\n",
        "embedding_path = f\"{base_path}_Embeds_{merge}\"\n",
        "\n",
        "torch_path  = f\"{embedding_path}.pt\"\n",
        "faiss_path  = f\"{embedding_path}.faiss\"\n",
        "mapping_path = f\"{embedding_path}_mapping.json\"\n",
        "mapping_data = f\"{embedding_path}_map_data.json\"\n",
        "\n",
        "FILE_TYPE    = file_type_val\n",
        "DATA_KEY     = data_key_val\n",
        "EMBE_KEY     = embe_key_val\n",
        "SWITCH       = switch\n",
        "EMBEDD_MODEL = embedding_model\n",
        "SEARCH_EGINE = searching_egine\n",
        "RERANK_MODEL = reranking_model\n",
        "RESPON_MODEL = responing_model\n",
        "\n",
        "if FILE_TYPE == \"Data\":\n",
        "    MERGE = merge\n",
        "else: \n",
        "    MERGE = \"no_Merge\"\n",
        "\n",
        "API_KEY = API_key_val\n",
        "\n",
        "SEARCH_ENGINE = faiss.IndexFlatIP\n",
        "\n",
        "print(\"\\n\")\n",
        "print(f\"Embedder: {EMBEDD_MODEL}\")\n",
        "print(f\"Searcher: {SEARCH_EGINE}\")\n",
        "print(f\"Reranker: {RERANK_MODEL}\")\n",
        "print(f\"Responer: {RESPON_MODEL}\")\n",
        "print(f\"Data Key: {DATA_KEY}\")\n",
        "print(f\"Embe Key: {EMBE_KEY}\")\n",
        "print(f\"Dcment  : {dcmt_path}\")\n",
        "print(f\"Chunked : {chunk_base}\")\n",
        "print(f\"Database: {json_file_path}\")\n",
        "print(f\"Torch   : {torch_path}\")\n",
        "print(f\"Faiss   : {faiss_path}\")\n",
        "print(f\"Mapping : {mapping_path}\")\n",
        "print(f\"Map Data: {mapping_data}\")\n",
        "print(f\"Schema  : {schema_ex_path}\")\n",
        "print(f\"Model   : {SWITCH}\")\n",
        "print(f\"Merge   : {MERGE}\")\n",
        "print(f\"API Key : {API_KEY}\")\n",
        "print(f\"Word    : {word_limit_val}\")\n",
        "print(f\"Level   : {level_input_val}\")\n",
        "print(f\"Level Values: {level_values}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = dcmt_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BASE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sentence_end(text):\n",
        "    brackets = [\"()\", \"''\", '\"\"', \"[]\", \"{}\", \"«»\", \"“”\", \"‘’\"]\n",
        "    valid_brackets = any(text.startswith(pair[0]) and text.endswith(pair[1]) for pair in brackets)\n",
        "    valid_end = text.endswith(('.', '!', '?', ':', ';'))\n",
        "    return valid_end or valid_brackets\n",
        "\n",
        "def markers(text):\n",
        "    return bool(re.match(r'^([-+*•●◦○] )|([0-9a-zA-Z\\-\\+\\*ivxIVX]+[.)\\]:] )|(\\(\\d+\\) )|(\\(\\w+\\) )|([0-9]+\\s+-\\s+[0-9]+ )', text))\n",
        "\n",
        "def unclosed(text):\n",
        "    stack = []\n",
        "    brackets = {\"(\": \")\", \"[\": \"]\", \"{\": \"}\", '\"': '\"', \"'\": \"'\", \"«\": \"»\", \"“\": \"”\", \"‘\": \"’\"}\n",
        "    for char in text:\n",
        "        if char in brackets.keys():\n",
        "            stack.append(char)\n",
        "        elif char in brackets.values():\n",
        "            if stack and brackets[stack[-1]] == char:\n",
        "                stack.pop()\n",
        "            else:\n",
        "                return False\n",
        "    return bool(stack)\n",
        "\n",
        "def merge_text(para, new_para):\n",
        "    should_merge = (not (new_para.isupper() ^ para.isupper()) and not markers(new_para) and (not new_para[0].isupper() or not sentence_end(para))) or unclosed(para)\n",
        "    return should_merge\n",
        "\n",
        "def extracted(path):\n",
        "    file_ext = os.path.splitext(path)[1].lower()\n",
        "    text_data = []\n",
        "\n",
        "    if file_ext == \".docx\":\n",
        "        doc = Document(path)\n",
        "        paragraph = \"\"\n",
        "        for para in doc.paragraphs:\n",
        "            for line in para.text.split(\"\\n\"):\n",
        "                cleaned_text = ' '.join(line.strip().split())\n",
        "                if cleaned_text:\n",
        "                    if paragraph and merge_text(paragraph, cleaned_text):\n",
        "                        paragraph += \" \" + cleaned_text\n",
        "                    else:\n",
        "                        if paragraph:\n",
        "                            text_data.append({\"text\": paragraph})\n",
        "                        paragraph = cleaned_text\n",
        "        if paragraph:\n",
        "            text_data.append({\"text\": paragraph})\n",
        "\n",
        "    elif file_ext == \".doc\":\n",
        "        word = win32com.client.Dispatch(\"Word.Application\")\n",
        "        word.Visible = False\n",
        "        doc = word.Documents.Open(os.path.abspath(path))\n",
        "        text = doc.Content.Text\n",
        "        doc.Close()\n",
        "        word.Quit()\n",
        "\n",
        "        paragraph = \"\"\n",
        "        for line in text.split(\"\\n\"):\n",
        "            cleaned_text = ' '.join(line.strip().split())\n",
        "            if cleaned_text:\n",
        "                if paragraph and merge_text(paragraph, cleaned_text):\n",
        "                    paragraph += \" \" + cleaned_text\n",
        "                else:\n",
        "                    if paragraph:\n",
        "                        text_data.append({\"text\": paragraph})\n",
        "                    paragraph = cleaned_text\n",
        "        if paragraph:\n",
        "            text_data.append({\"text\": paragraph})\n",
        "\n",
        "    elif file_ext == \".pdf\":\n",
        "        doc = fitz.open(path)\n",
        "        paragraph = \"\"\n",
        "        for page in doc:\n",
        "            blocks = sorted(page.get_text(\"blocks\"), key=lambda b: (b[1], b[0]))\n",
        "            for block in blocks:\n",
        "                for line in block[4].split(\"\\n\"):\n",
        "                    cleaned_text = \" \".join(line.strip().split())\n",
        "                    if cleaned_text:\n",
        "                        if paragraph and merge_text(paragraph, cleaned_text):\n",
        "                            paragraph += \" \" + cleaned_text\n",
        "                        else:\n",
        "                            if paragraph:\n",
        "                                text_data.append({\"text\": paragraph})\n",
        "                            paragraph = cleaned_text\n",
        "        if paragraph:\n",
        "            text_data.append({\"text\": paragraph})\n",
        "\n",
        "    return text_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ADD CHUNKS\n",
        "def add_chunk(chunks, content):\n",
        "    if content[\"Chương\"] and content[\"Nội dung\"]:\n",
        "        content[\"Index\"] += 1\n",
        "        chunks.append(content.copy())\n",
        "        content[\"Nội dung\"] = []\n",
        "        \n",
        "def is_chapter(text):\n",
        "    text = text.strip()\n",
        "    return bool(re.match(r\"^Chương\\s*[IVXLCDM\\d]+\\b\", text, re.IGNORECASE))\n",
        "\n",
        "def is_article(text):\n",
        "    text = text.strip()\n",
        "    return bool(re.match(r\"^Điều\\s+([IVXLCDM\\d]+)\\.\\s*(.+)\", text, re.IGNORECASE))\n",
        "\n",
        "def is_clause(text):\n",
        "    text = text.strip()\n",
        "    return bool(re.match(r\"^\\d+\\.\\s+.+\", text))\n",
        "\n",
        "def is_content(text):\n",
        "    text = text.strip()\n",
        "    return bool(re.match(r'^([-+*•●◦○] )|([a-zA-Z\\-\\+\\*]+[.)\\]:] )|(\\(\\w+\\) )', text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MAIN FUNCTION\n",
        "def main(text_data):\n",
        "    chunks = []\n",
        "    content = {\"Index\": 0, \"Chương\": None, \"Điều\": None, \"Khoản\": None, \"Nội dung\": []}\n",
        "    i = 0\n",
        "    while i < len(text_data):\n",
        "        chunk = text_data[i][\"text\"]\n",
        "\n",
        "        if is_chapter(chunk):\n",
        "            if i + 1 < len(text_data):\n",
        "                chunk += f\": {text_data[i + 1]['text']}\"\n",
        "            add_chunk(chunks, content)\n",
        "            content[\"Chương\"] = chunk\n",
        "            content[\"Điều\"] = None\n",
        "            content[\"Khoản\"] = None \n",
        "            i += 1\n",
        "\n",
        "        elif is_article(chunk):\n",
        "            match = re.match(r\"^(Điều\\s*[IVXLCDM\\d]+)\\.\\s*(.+)\", chunk, re.IGNORECASE)\n",
        "            if content[\"Chương\"]:\n",
        "                if match:\n",
        "                    chunk = f\"{match.group(1)}: {match.group(2)}\"\n",
        "                add_chunk(chunks, content)\n",
        "                content[\"Điều\"] = chunk\n",
        "                content[\"Khoản\"] = None \n",
        "        \n",
        "        elif is_clause(chunk):\n",
        "            match = re.match(r\"^(\\d+)\\.\\s*(.+)\", chunk)\n",
        "            if content[\"Chương\"]:\n",
        "                if match:\n",
        "                    clause_number = match.group(1)\n",
        "                    clause_content = match.group(2)\n",
        "\n",
        "                    if i + 1 < len(text_data) and is_content(text_data[i + 1][\"text\"]):\n",
        "                        chunk = f\"Khoản {clause_number}: {clause_content}\"\n",
        "                        add_chunk(chunks, content)\n",
        "                        content[\"Khoản\"] = chunk\n",
        "                    else:\n",
        "                        chunk = f\"Khoản {clause_number}\"\n",
        "                        add_chunk(chunks, content)\n",
        "                        content[\"Khoản\"] = chunk\n",
        "\n",
        "                        chunk = clause_content\n",
        "                        content[\"Nội dung\"].append(chunk)\n",
        "                else: \n",
        "                    print(chunk)\n",
        "\n",
        "        elif is_content(chunk):\n",
        "            match = re.match(r'^([-+*•●◦○a-zA-Z\\-\\+\\*]+[.)\\]:] )(\\s.+)', chunk)\n",
        "            if content[\"Chương\"]:\n",
        "                if match:\n",
        "                    chunk = match.group(2)\n",
        "                content[\"Nội dung\"].append(chunk)\n",
        "        i += 1\n",
        "        \n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CHUNKS BASE\n",
        "text_data = extracted(dcmt_path)\n",
        "chunks = main(text_data)\n",
        "with open(\"extract_text.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(text_data, f, indent=4, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXPORT BASE\n",
        "with open(chunks_base, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(chunks, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"Base data saved to {chunks_base}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "FINAL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "WORD_LIMIT = int(word_input.value) if word_input.value.isdigit() else 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# COUNT WORDS\n",
        "def count_words(text):\n",
        "    return len(text.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CHUNKING IF WORD LIMIT EXCEEDED\n",
        "def semantic_chunking(text, max_words=WORD_LIMIT):\n",
        "\n",
        "    doc = nlp(text)\n",
        "    chunks, current_chunk = [], []\n",
        "    word_count = 0\n",
        "    \n",
        "    for sent in doc.sents:\n",
        "        sentence = sent.text.strip()\n",
        "        sentence_length = len(sentence.split())\n",
        "        \n",
        "        if word_count + sentence_length > max_words and current_chunk:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = []\n",
        "            word_count = 0\n",
        "            \n",
        "        current_chunk.append(sentence)\n",
        "        word_count += sentence_length\n",
        "        \n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "        \n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MAIN PROCESSING FUNCTION\n",
        "def process_json(chunks_base, chunks_final):\n",
        "    with open(chunks_base, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "        \n",
        "    processed_data = []\n",
        "    \n",
        "    for idx, chunk in enumerate(data):\n",
        "        \n",
        "        # PRINT PROGRESS INFO\n",
        "        if \"Nội dung\" in chunk and isinstance(chunk[\"Nội dung\"], list):\n",
        "            new_content = []\n",
        "            \n",
        "            for para_idx, paragraph in enumerate(chunk[\"Nội dung\"]):\n",
        "                word_count = count_words(paragraph)\n",
        "                \n",
        "                if word_count > WORD_LIMIT  and not markers(paragraph):\n",
        "                    \n",
        "                    # WORDS NUMBER                    \n",
        "                    chunked_paragraphs = semantic_chunking(paragraph)\n",
        "                    new_content.extend(chunked_paragraphs)\n",
        "                    \n",
        "                    # PRINT SEGMENTS NUMBER\n",
        "                    print(f\"{idx+1:04} / {len(data):04}: {len(chunked_paragraphs):02} segments.\")\n",
        "                    \n",
        "                else:\n",
        "                    new_content.append(paragraph)\n",
        "                    \n",
        "            chunk[\"Nội dung\"] = new_content\n",
        "            \n",
        "        processed_data.append(chunk)\n",
        "        \n",
        "        # SAVE PROGRESS\n",
        "        with open(chunks_final, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(processed_data, f, indent=4, ensure_ascii=False)\n",
        "        print(f\"{idx+1:04} / {len(data):04}: Saved!\\n\")\n",
        "    \n",
        "    # FINISHED    \n",
        "    print(f\"Final data saved to {chunks_final}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "process_json(chunks_base, chunks_final)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
