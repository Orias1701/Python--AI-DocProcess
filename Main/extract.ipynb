{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee3bcad2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from docx import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356565df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import win32com.client\n",
    "import fitz\n",
    "import spacy\n",
    "from langdetect import detect\n",
    "import logging\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from statistics import mean\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b31540",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "nlp = spacy.load(\"xx_ent_wiki_sm\")\n",
    "\n",
    "def sentence_end(text):\n",
    "    \"\"\"\n",
    "    Kiểm tra xem chuỗi văn bản có kết thúc bằng dấu câu hợp lệ hoặc cặp ngoặc hợp lệ hay không.\n",
    "    \"\"\"\n",
    "    brackets = [\"()\", \"''\", '\"\"', \"[]\", \"{}\", \"«»\", \"“”\", \"‘’\"]\n",
    "    valid_brackets = any(text.startswith(pair[0]) and text.endswith(pair[1]) for pair in brackets)\n",
    "    valid_end = text.endswith(('.', '!', '?', ':', ';'))\n",
    "    return valid_end or valid_brackets\n",
    "\n",
    "def markers(text):\n",
    "    \"\"\"\n",
    "    Kiểm tra xem chuỗi văn bản có khớp với cấu trúc tiêu đề hay không.\n",
    "    \"\"\"\n",
    "    patterns = [\n",
    "        r\"^[A-ZÀ-Ỹa-zà-ỹ]+\\s+[0-9IVXLCDM]+[:.\\s].*\",  # Chương 1, Điều 1, Section I\n",
    "        r\"^[0-9]+[\\.\\)]\\s*.*\",  # 1., 1)\n",
    "        r\"^[IVXLCDM]+[\\.\\)]\\s*.*\",  # I., II)\n",
    "        r\"^[a-zA-Z][\\.\\)]\\s*.*\",  # a., b)\n",
    "        r\"^[0-9]+\\.[0-9]+[\\.\\)]\\s*.*\",  # 1.1, 1.2\n",
    "        r\"^[A-ZÀ-Ỹ][A-ZÀ-Ỹ\\s]{2,}$\",  # QUY ĐỊNH CHUNG\n",
    "    ]\n",
    "    return any(re.match(pattern, text, re.IGNORECASE) for pattern in patterns)\n",
    "\n",
    "def is_title_by_nlp(text, prev_text):\n",
    "    \"\"\"\n",
    "    Sử dụng NLP để xác định dòng có phải tiêu đề hay không.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    if len(doc) > 10 or any(token.pos_ == \"VERB\" for token in doc):\n",
    "        return False\n",
    "    return len(doc) < 10 and any(token.pos_ in [\"NOUN\", \"PROPN\"] for token in doc)\n",
    "\n",
    "def unclosed(text):\n",
    "    \"\"\"\n",
    "    Kiểm tra xem chuỗi văn bản có chứa ngoặc chưa được đóng hay không.\n",
    "    \"\"\"\n",
    "    stack = []\n",
    "    brackets = {\"(\": \")\", \"[\": \"]\", \"{\": \"}\", '\"': '\"', \"'\": \"'\", \"«\": \"»\", \"“\": \"”\", \"‘\": \"’\"}\n",
    "    for char in text:\n",
    "        if char in brackets.keys():\n",
    "            stack.append(char)\n",
    "        elif char in brackets.values():\n",
    "            if stack and brackets[stack[-1]] == char:\n",
    "                stack.pop()\n",
    "            else:\n",
    "                return False\n",
    "    return bool(stack)\n",
    "\n",
    "def merge_text(para, new_para, is_para_title, is_new_para_title, prev_text):\n",
    "    \"\"\"\n",
    "    Quyết định xem có nên gộp hai đoạn văn bản thành một hay không.\n",
    "    \"\"\"\n",
    "    if is_new_para_title or is_para_title:\n",
    "        return False\n",
    "    return (\n",
    "        not markers(new_para) and\n",
    "        (not new_para[0].isupper() or not sentence_end(para)) or\n",
    "        unclosed(para)\n",
    "    )\n",
    "\n",
    "def extract_text_from_image_page(page):\n",
    "    \"\"\"\n",
    "    Trích xuất văn bản từ trang PDF quét bằng OCR.\n",
    "    \"\"\"\n",
    "    pix = page.get_pixmap()\n",
    "    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.rgb)\n",
    "    return pytesseract.image_to_string(img, lang=\"vie+eng\")\n",
    "\n",
    "def learn_title_hierarchy(doc):\n",
    "    \"\"\"\n",
    "    Duyệt văn bản để xác định cấp độ tiêu đề dựa trên đoạn có nhiều cấp nhất.\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    for page in doc:\n",
    "        blocks = sorted(page.get_text(\"blocks\"), key=lambda b: (b[1], b[0]))\n",
    "        segment = []\n",
    "        prev_text = \"\"\n",
    "        prev_block = None\n",
    "        for block in blocks:\n",
    "            font_info = block[6] if len(block) > 6 else {}\n",
    "            x0 = block[0]\n",
    "            spacing = (block[1] - prev_block[3] if prev_block else 0)\n",
    "            is_isolated = spacing > 10\n",
    "            for line in block[4].split(\"\\n\"):\n",
    "                cleaned_text = \" \".join(line.strip().split())\n",
    "                if cleaned_text:\n",
    "                    doc = nlp(cleaned_text)\n",
    "                    is_content = len(doc) > 10 or any(token.pos_ == \"VERB\" for token in doc)\n",
    "                    if is_content or not is_isolated:\n",
    "                        continue\n",
    "                    if markers(cleaned_text) or is_title_by_nlp(cleaned_text, prev_text):\n",
    "                        segment.append({\n",
    "                            \"text\": cleaned_text,\n",
    "                            \"indent\": x0,\n",
    "                            \"font_size\": font_info.get(\"size\", 10),\n",
    "                            \"bold\": font_info.get(\"weight\", \"\").lower() == \"bold\",\n",
    "                            \"italic\": font_info.get(\"flags\", 0) & 2 != 0,\n",
    "                            \"alignment\": \"center\" if abs(block[0] - block[2]) < abs(block[1] - block[3]) else \"left\"\n",
    "                        })\n",
    "                prev_text = cleaned_text\n",
    "                prev_block = block\n",
    "        if segment:\n",
    "            segments.append(segment)\n",
    "    \n",
    "    # Tìm đoạn có nhiều cấp nhất\n",
    "    max_levels = 0\n",
    "    selected_segment = None\n",
    "    for segment in segments:\n",
    "        indents = set(item[\"indent\"] for item in segment)\n",
    "        if len(indents) > max_levels:\n",
    "            max_levels = len(indents)\n",
    "            selected_segment = segment\n",
    "    \n",
    "    if not selected_segment:\n",
    "        return {}\n",
    "    \n",
    "    # Xác định cấp độ dựa trên lùi đầu dòng và font size\n",
    "    title_formats = {}\n",
    "    current_level = 1\n",
    "    seen_patterns = set()\n",
    "    sorted_segment = sorted(selected_segment, key=lambda x: (x[\"indent\"], -x[\"font_size\"]))\n",
    "    for item in sorted_segment:\n",
    "        pattern = re.match(r\"^[A-ZÀ-Ỹa-zà-ỹ]+\\s+[0-9IVXLCDM]+|[0-9a-zA-ZIVXLCDM]+[\\.\\)]|[A-ZÀ-Ỹ\\s]{2,}\", item[\"text\"], re.IGNORECASE)\n",
    "        if pattern:\n",
    "            pattern_key = pattern.group(0)\n",
    "            if pattern_key not in seen_patterns:\n",
    "                seen_patterns.add(pattern_key)\n",
    "                title_formats[f\"level_{current_level}\"] = {\n",
    "                    \"pattern\": pattern_key,\n",
    "                    \"font_size\": item[\"font_size\"],\n",
    "                    \"bold\": item[\"bold\"],\n",
    "                    \"italic\": item[\"italic\"],\n",
    "                    \"alignment\": item[\"alignment\"],\n",
    "                    \"indent\": item[\"indent\"],\n",
    "                    \"count\": 1\n",
    "                }\n",
    "                current_level += 1\n",
    "    \n",
    "    # Gộp định dạng từ các đoạn khác\n",
    "    for segment in segments:\n",
    "        for item in segment:\n",
    "            for level, fmt in title_formats.items():\n",
    "                if re.match(rf\"^{fmt['pattern']}\", item[\"text\"], re.IGNORECASE):\n",
    "                    fmt[\"font_size\"] = (fmt[\"font_size\"] * fmt[\"count\"] + item[\"font_size\"]) / (fmt[\"count\"] + 1)\n",
    "                    fmt[\"count\"] += 1\n",
    "    \n",
    "    return title_formats\n",
    "\n",
    "def is_title(block, text, prev_block, prev_text, title_formats):\n",
    "    \"\"\"\n",
    "    Xác định dòng có phải tiêu đề dựa trên định dạng và cấp độ đã học.\n",
    "    \"\"\"\n",
    "    if not (markers(text) or is_title_by_nlp(text, prev_text)):\n",
    "        return False\n",
    "    \n",
    "    font_info = block[6] if len(block) > 6 else {}\n",
    "    font_size = font_info.get(\"size\", 10)\n",
    "    is_bold = font_info.get(\"weight\", \"\").lower() == \"bold\"\n",
    "    is_italic = font_info.get(\"flags\", 0) & 2 != 0\n",
    "    alignment = \"center\" if abs(block[0] - block[2]) < abs(block[1] - block[3]) else \"left\"\n",
    "    indent = block[0]\n",
    "    spacing = (block[1] - prev_block[3] if prev_block else 0)\n",
    "    is_isolated = spacing > 10\n",
    "    \n",
    "    if not is_isolated:\n",
    "        return False\n",
    "    \n",
    "    for level, fmt in title_formats.items():\n",
    "        if re.match(rf\"^{fmt['pattern']}\", text, re.IGNORECASE):\n",
    "            font_size_match = abs(font_size - fmt[\"font_size\"]) <= 1\n",
    "            bold_match = is_bold == fmt[\"bold\"]\n",
    "            italic_match = is_italic == fmt[\"italic\"]\n",
    "            alignment_match = alignment == fmt[\"alignment\"]\n",
    "            indent_match = abs(indent - fmt[\"indent\"]) <= 5\n",
    "            if sum([font_size_match, bold_match, italic_match, alignment_match, indent_match]) >= 5:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def validate_segments(text_data):\n",
    "    \"\"\"\n",
    "    Kiểm tra và gộp các đoạn ngắn bất thường.\n",
    "    \"\"\"\n",
    "    validated_data = []\n",
    "    for i, segment in enumerate(text_data):\n",
    "        if len(segment[\"text\"].split()) < 5 and not segment[\"metadata\"][\"is_title\"]:\n",
    "            if i > 0 and not text_data[i-1][\"metadata\"][\"is_title\"]:\n",
    "                validated_data[-1][\"text\"] += \" \" + segment[\"text\"]\n",
    "                continue\n",
    "        validated_data.append(segment)\n",
    "    return validated_data\n",
    "\n",
    "def extracted(path):\n",
    "    \"\"\"\n",
    "    Trích xuất văn bản từ file (.docx, .doc, .pdf) và tổ chức thành các đoạn.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file_ext = os.path.splitext(path)[1].lower()\n",
    "        text_data = []\n",
    "\n",
    "        if file_ext == \".docx\":\n",
    "            doc = Document(path)\n",
    "            paragraph = \"\"\n",
    "            is_para_title = False\n",
    "            prev_text = \"\"\n",
    "            for para in doc.paragraphs:\n",
    "                for line in para.text.split(\"\\n\"):\n",
    "                    cleaned_text = ' '.join(line.strip().split())\n",
    "                    if cleaned_text:\n",
    "                        is_new_para_title = markers(cleaned_text) or is_title_by_nlp(cleaned_text, prev_text)\n",
    "                        if paragraph and merge_text(paragraph, cleaned_text, is_para_title, is_new_para_title, prev_text):\n",
    "                            paragraph += \" \" + cleaned_text\n",
    "                        else:\n",
    "                            if paragraph:\n",
    "                                text_data.append({\"text\": paragraph, \"metadata\": {\"is_title\": is_para_title}})\n",
    "                            paragraph = cleaned_text\n",
    "                            is_para_title = is_new_para_title\n",
    "                        prev_text = cleaned_text\n",
    "            if paragraph:\n",
    "                text_data.append({\"text\": paragraph, \"metadata\": {\"is_title\": is_para_title}})\n",
    "            return validate_segments(text_data)\n",
    "\n",
    "        elif file_ext == \".doc\":\n",
    "            word = win32com.client.Dispatch(\"Word.Application\")\n",
    "            word.Visible = False\n",
    "            doc = word.Documents.Open(os.path.abspath(path))\n",
    "            text = doc.Content.Text\n",
    "            doc.Close()\n",
    "            word.Quit()\n",
    "\n",
    "            paragraph = \"\"\n",
    "            is_para_title = False\n",
    "            prev_text = \"\"\n",
    "            for line in text.split(\"\\n\"):\n",
    "                cleaned_text = ' '.join(line.strip().split())\n",
    "                if cleaned_text:\n",
    "                    is_new_para_title = markers(cleaned_text) or is_title_by_nlp(cleaned_text, prev_text)\n",
    "                    if paragraph and merge_text(paragraph, cleaned_text, is_para_title, is_new_para_title, prev_text):\n",
    "                        paragraph += \" \" + cleaned_text\n",
    "                    else:\n",
    "                        if paragraph:\n",
    "                            text_data.append({\"text\": paragraph, \"metadata\": {\"is_title\": is_para_title}})\n",
    "                        paragraph = cleaned_text\n",
    "                        is_para_title = is_new_para_title\n",
    "                    prev_text = cleaned_text\n",
    "            if paragraph:\n",
    "                text_data.append({\"text\": paragraph, \"metadata\": {\"is_title\": is_para_title}})\n",
    "            return validate_segments(text_data)\n",
    "\n",
    "        elif file_ext == \".pdf\":\n",
    "            doc = fitz.open(path)\n",
    "            sample_text = \" \".join(page.get_text(\"text\") for page in doc[:2] if page.get_text(\"text\").strip())\n",
    "            language = detect(sample_text) if sample_text else \"unknown\"\n",
    "            \n",
    "            # Học cấp độ và định dạng tiêu đề\n",
    "            title_formats = learn_title_hierarchy(doc)\n",
    "            logging.info(f\"Learned title formats: {title_formats}\")\n",
    "            \n",
    "            paragraph = \"\"\n",
    "            is_para_title = False\n",
    "            prev_text = \"\"\n",
    "            prev_block = None\n",
    "            for page in doc:\n",
    "                if not page.get_text(\"text\").strip():\n",
    "                    text = extract_text_from_image_page(page)\n",
    "                    blocks = [(0, 0, 0, 0, text, 0, {})]\n",
    "                else:\n",
    "                    blocks = sorted(page.get_text(\"blocks\"), key=lambda b: (b[1], b[0]))\n",
    "                for i, block in enumerate(blocks):\n",
    "                    font_info = block[6] if len(block) > 6 else {}\n",
    "                    next_block = blocks[i+1] if i+1 < len(blocks) else None\n",
    "                    metadata = {\n",
    "                        \"page\": page.number,\n",
    "                        \"font_size\": font_info.get(\"size\", 10),\n",
    "                        \"is_bold\": font_info.get(\"weight\", \"\").lower() == \"bold\",\n",
    "                        \"x0\": block[0],\n",
    "                        \"y0\": block[1],\n",
    "                        \"language\": language\n",
    "                    }\n",
    "                    for line in block[4].split(\"\\n\"):\n",
    "                        cleaned_text = \" \".join(line.strip().split())\n",
    "                        if cleaned_text:\n",
    "                            is_new_para_title = is_title(block, cleaned_text, prev_block, prev_text, title_formats)\n",
    "                            metadata[\"is_title\"] = is_new_para_title\n",
    "                            if paragraph and merge_text(paragraph, cleaned_text, is_para_title, is_new_para_title, prev_text):\n",
    "                                paragraph += \" \" + cleaned_text\n",
    "                            else:\n",
    "                                if paragraph:\n",
    "                                    text_data.append({\"text\": paragraph, \"metadata\": metadata})\n",
    "                                paragraph = cleaned_text\n",
    "                                is_para_title = is_new_para_title\n",
    "                            prev_text = cleaned_text\n",
    "                    prev_block = block\n",
    "            if paragraph:\n",
    "                text_data.append({\"text\": paragraph, \"metadata\": metadata})\n",
    "            return validate_segments(text_data)\n",
    "\n",
    "        return text_data\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {path}: {e}\")\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ce7058",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    chunks = extracted(\"../Doc/HNMU_Regulations.pdf\")\n",
    "    import json\n",
    "    with open(\"output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunks, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chunk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
