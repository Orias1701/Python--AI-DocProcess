{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz, os, faiss\n",
    "import re\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from Config import Configs\n",
    "from Config import ModelLoader as ML\n",
    "from Libraries import Common_MyUtils as MU, Common_TextProcess as TP\n",
    "from Libraries import PDF_ExtractData as ExtractData, PDF_MergeData as MergeData, Json_ChunkUnder as ChunkUnder\n",
    "from Libraries import Faiss_Embedding as F_Embedding, Faiss_Searching as F_Searching\n",
    "from sentence_transformers import CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Checkpoint = \"vinai/bartpho-syllable\"\n",
    "service = \"Categories\"\n",
    "inputs = \"VH.pdf\"\n",
    "JsonKey = \"paragraphs\"\n",
    "JsonField = \"Text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Configs.ConfigValues(service=service, inputs=inputs)\n",
    "inputPath = config[\"inputPath\"]\n",
    "PdfPath = config[\"PdfPath\"]\n",
    "DocPath = config[\"DocPath\"]\n",
    "exceptPath = config[\"exceptPath\"]\n",
    "markerPath = config[\"markerPath\"]\n",
    "statusPath = config[\"statusPath\"]\n",
    "RawDataPath = config[\"RawDataPath\"]\n",
    "RawLvlsPath = config[\"RawLvlsPath\"]\n",
    "StructsPath = config[\"StructsPath\"]\n",
    "SegmentPath = config[\"SegmentPath\"]\n",
    "SchemaPath = config[\"SchemaPath\"]\n",
    "FaissPath = config[\"FaissPath\"]\n",
    "MappingPath = config[\"MappingPath\"]\n",
    "MapDataPath = config[\"MapDataPath\"]\n",
    "MapChunkPath = config[\"MapChunkPath\"]\n",
    "MetaPath = config[\"MetaPath\"]\n",
    "DATA_KEY = config[\"DATA_KEY\"]\n",
    "EMBE_KEY = config[\"EMBE_KEY\"]\n",
    "SEARCH_EGINE = config[\"SEARCH_EGINE\"]\n",
    "RERANK_MODEL = config[\"RERANK_MODEL\"]\n",
    "RESPON_MODEL = config[\"RESPON_MODEL\"]\n",
    "EMBEDD_MODEL = config[\"EMBEDD_MODEL\"]\n",
    "CHUNKS_MODEL = config[\"CHUNKS_MODEL\"]\n",
    "SUMARY_MODEL = config[\"SUMARY_MODEL\"]\n",
    "WORD_LIMIT = config[\"WORD_LIMIT\"]\n",
    "\n",
    "MODEL_DIR = \"Models\"\n",
    "MODEL_TYPE = \"Sentence_Transformer\"\n",
    "EMBEDD_CACHED_MODEL = f\"{MODEL_DIR}/{MODEL_TYPE}/{EMBEDD_MODEL}\"\n",
    "CHUNKS_CACHED_MODEL = F\"{MODEL_DIR}/{MODEL_TYPE}/{CHUNKS_MODEL}\"\n",
    "SUMARY_CACHED_MODEL = f\"{MODEL_DIR}/{MODEL_TYPE}/{SUMARY_MODEL}\"\n",
    "\n",
    "MAX_INPUT = 1024\n",
    "MAX_TARGET = 256\n",
    "MIN_TARGET = 64\n",
    "TRAIN_EPOCHS = 3\n",
    "LEARNING_RATE = 3e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadHardcodes(file_path, wanted=None):\n",
    "    data = MU.read_json(file_path)\n",
    "    if \"items\" not in data:\n",
    "        return\n",
    "    result = {}\n",
    "    for item in data[\"items\"]:\n",
    "        key = item[\"key\"]\n",
    "        if (not wanted) or (key in wanted):\n",
    "            result[key] = item[\"values\"]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptData = loadHardcodes(exceptPath, wanted=[\"common_words\", \"proper_names\", \"abbreviations\"])\n",
    "markerData = loadHardcodes(markerPath, wanted=[\"keywords\", \"markers\"])\n",
    "statusData = loadHardcodes(statusPath, wanted=[\"brackets\", \"sentence_ends\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer, embeddDevice = ML.init_sentence_model(EMBEDD_MODEL, EMBEDD_CACHED_MODEL)\n",
    "chunker, chunksDevice = ML.init_sentence_model(CHUNKS_MODEL, CHUNKS_CACHED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mapping = MU.read_json(MappingPath)\n",
    "MapData = MU.read_json(MapDataPath)\n",
    "MapChunk = MU.read_json(MapChunkPath)\n",
    "faissIndex = faiss.read_index(FaissPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataExtractor = ExtractData.B1Extractor(\n",
    "    exceptData,\n",
    "    markerData,\n",
    "    statusData,\n",
    "    proper_name_min_count=10\n",
    ")\n",
    "\n",
    "chunkUnder = ChunkUnder.ChunkUndertheseaBuilder(\n",
    "    embedder=chunker,\n",
    "    min_words=512,\n",
    "    max_words=1024,\n",
    "    sim_threshold=0.7\n",
    ")\n",
    "\n",
    "reranker = CrossEncoder(RERANK_MODEL, device=str(embeddDevice))\n",
    "engine = F_Searching.SemanticSearchEngine(\n",
    "    indexer=indexer,\n",
    "    reranker=reranker,\n",
    "    device=str(embeddDevice),\n",
    "    normalize=True,\n",
    "    top_k=20,\n",
    "    rerank_k=10,\n",
    "    rerank_batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractRun(pdf_doc):\n",
    "    extractedData = dataExtractor.extract(pdf_doc)\n",
    "    RawDataDict = MergeData.mergeLinesToParagraphs(extractedData)\n",
    "    return RawDataDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelTest(sample_text=None, ModelPath=None, max_length=256, min_length=64):\n",
    "    summarizer_pipeline = pipeline(\"summarization\", model=ModelPath)\n",
    "    summary = summarizer_pipeline(\n",
    "        sample_text,\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "        do_sample=False,\n",
    "        num_beams=4,\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return summary[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_recursive(text, depth=0, max_depth=5):\n",
    "    word_count = len(text.split())\n",
    "    indent = \"  \" * depth\n",
    "    print(f\"{indent}ðŸ”¹ Level {depth}: {word_count} tá»«\")\n",
    "\n",
    "    if word_count < 512:\n",
    "        return text\n",
    "    elif word_count < 1024:\n",
    "        return modelTest(text, SUMARY_CACHED_MODEL, MAX_TARGET, MIN_TARGET)\n",
    "    else:\n",
    "        chunks = chunkUnder.build(text)\n",
    "        summaries = []\n",
    "\n",
    "        for item in chunks:\n",
    "            content = item[\"Content\"]\n",
    "            idx = item.get(\"Index\", \"?\")\n",
    "            print(f\"{indent}  ðŸ”¸ Chunk {idx}: {len(content.split())} tá»«\")\n",
    "            try:\n",
    "                sub_summary = modelTest(content, SUMARY_CACHED_MODEL, MAX_TARGET, MIN_TARGET)\n",
    "            except Exception as e:\n",
    "                return \"Bruh\"\n",
    "            \n",
    "            summaries.append(sub_summary)\n",
    "\n",
    "        merged_summary = \"\\n\".join(summaries)\n",
    "        merged_len = len(merged_summary.split())\n",
    "        if merged_len > 1024 and depth < max_depth:\n",
    "            return summarize_recursive(merged_summary, depth + 1, max_depth)\n",
    "        \n",
    "        return merged_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runSearch(query):\n",
    "    results = engine.search(\n",
    "        query=query,\n",
    "        faissIndex=faissIndex,\n",
    "        Mapping=Mapping,\n",
    "        MapData=MapData,\n",
    "        top_k=20\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runRerank(query, results):\n",
    "    reranked = engine.rerank(\n",
    "        query=query,\n",
    "        results=results,\n",
    "        top_k=10\n",
    "    )\n",
    "    return reranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_doc = fitz.open(inputPath)\n",
    "RawDataDict = extractRun(pdf_doc)\n",
    "pdf_doc.close()\n",
    "full_text = TP.merge_txt(RawDataDict, JsonKey, JsonField)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(full_text.split()) > 512:\n",
    "    final_summary = summarize_recursive(text=full_text)\n",
    "else:\n",
    "    final_summary = modelTest(full_text, SUMARY_CACHED_MODEL, MAX_TARGET, MIN_TARGET)\n",
    "print(\"\\nâœ¨ FINAL SUMMARY âœ¨\\n\")\n",
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resuls = runSearch(final_summary)\n",
    "reranked = runRerank(final_summary, resuls)\n",
    "\n",
    "best_text = reranked[0][\"text\"] if reranked else \"\"\n",
    "print(best_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
