{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz, os, faiss\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "from Config import Configs\n",
    "from Config import ModelLoader as ML\n",
    "\n",
    "from Libraries import Common_MyUtils as MU\n",
    "from Libraries import PDF_ExtractData as ExtractData, PDF_MergeData as MergeData\n",
    "from Libraries import Json_GetStructures as GetStructures, Json_ChunkMaster as ChunkMaster, Json_SchemaExt as SchemaExt\n",
    "from Libraries import Faiss_Embedding as F_Embedding, Faiss_Searching as F_Searching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIGURATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SERVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Checkpoint = \"vinai/bartpho-syllable\"\n",
    "service = \"HNMU\"\n",
    "inputs = \"HNMU.pdf\"\n",
    "JsonKey = \"paragraphs\"\n",
    "JsonField = \"Text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PATHS & MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Configs.ConfigValues(service=service, inputs=inputs)\n",
    "inputPath = config[\"inputPath\"]\n",
    "PdfPath = config[\"PdfPath\"]\n",
    "DocPath = config[\"DocPath\"]\n",
    "exceptPath = config[\"exceptPath\"]\n",
    "markerPath = config[\"markerPath\"]\n",
    "statusPath = config[\"statusPath\"]\n",
    "RawDataPath = config[\"RawDataPath\"]\n",
    "RawLvlsPath = config[\"RawLvlsPath\"]\n",
    "StructsPath = config[\"StructsPath\"]\n",
    "SegmentPath = config[\"SegmentPath\"]\n",
    "SchemaPath = config[\"SchemaPath\"]\n",
    "FaissPath = config[\"FaissPath\"]\n",
    "MappingPath = config[\"MappingPath\"]\n",
    "MapDataPath = config[\"MapDataPath\"]\n",
    "MapChunkPath = config[\"MapChunkPath\"]\n",
    "MetaPath = config[\"MetaPath\"]\n",
    "DATA_KEY = config[\"DATA_KEY\"]\n",
    "EMBE_KEY = config[\"EMBE_KEY\"]\n",
    "SEARCH_EGINE = config[\"SEARCH_EGINE\"]\n",
    "RERANK_MODEL = config[\"RERANK_MODEL\"]\n",
    "RESPON_MODEL = config[\"RESPON_MODEL\"]\n",
    "EMBEDD_MODEL = config[\"EMBEDD_MODEL\"]\n",
    "CHUNKS_MODEL = config[\"CHUNKS_MODEL\"]\n",
    "SUMARY_MODEL = config[\"SUMARY_MODEL\"]\n",
    "WORD_LIMIT = config[\"WORD_LIMIT\"]\n",
    "\n",
    "MODEL_DIR = \"Models\"\n",
    "MODEL_TYPE = \"Sentence_Transformer\"\n",
    "EMBEDD_CACHED_MODEL = f\"{MODEL_DIR}/{MODEL_TYPE}/{EMBEDD_MODEL}\"\n",
    "CHUNKS_CACHED_MODEL = F\"{MODEL_DIR}/{MODEL_TYPE}/{CHUNKS_MODEL}\"\n",
    "SUMARY_CACHED_MODEL = f\"{MODEL_DIR}/{MODEL_TYPE}/{SUMARY_MODEL}\"\n",
    "\n",
    "MAX_INPUT = 1024\n",
    "MAX_TARGET = 256\n",
    "MIN_TARGET = 64\n",
    "TRAIN_EPOCHS = 3\n",
    "LEARNING_RATE = 3e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "BATCH_SIZE = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HELPERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadHardcodes(file_path, wanted=None):\n",
    "    data = MU.read_json(file_path)\n",
    "    if \"items\" not in data:\n",
    "        return\n",
    "    result = {}\n",
    "    for item in data[\"items\"]:\n",
    "        key = item[\"key\"]\n",
    "        if (not wanted) or (key in wanted):\n",
    "            result[key] = item[\"values\"]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptData = loadHardcodes(exceptPath, wanted=[\"common_words\", \"proper_names\", \"abbreviations\"])\n",
    "markerData = loadHardcodes(markerPath, wanted=[\"keywords\", \"markers\"])\n",
    "statusData = loadHardcodes(statusPath, wanted=[\"brackets\", \"sentence_ends\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer, embeddDevice = ML.init_sentence_model(EMBEDD_MODEL, EMBEDD_CACHED_MODEL)\n",
    "chunker, chunksDevice = ML.init_sentence_model(CHUNKS_MODEL, CHUNKS_CACHED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPARING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXTRACT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataExtractor = ExtractData.B1Extractor(\n",
    "    exceptData,\n",
    "    markerData,\n",
    "    statusData,\n",
    "    proper_name_min_count=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GET STRUCTURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structAnalyzer = GetStructures.StructureAnalyzer(\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BUILD CHUNKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkBuilder = ChunkMaster.ChunkBuilder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCHEMA EXTRACTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaExt = SchemaExt.JSONSchemaExtractor(\n",
    "    list_policy=\"first\", \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INDEXING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faissIndexer = F_Embedding.DirectFaissIndexer(\n",
    "    indexer=indexer,\n",
    "    device=str(embeddDevice),\n",
    "    batch_size=32,\n",
    "    show_progress=True,\n",
    "    flatten_mode=\"split\",\n",
    "    join_sep=\"\\n\",\n",
    "    allowed_schema_types=(\"string\", \"array\", \"dict\"),\n",
    "    max_chars_per_text=2000,\n",
    "    normalize=True,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXTRACT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractRun(pdf_doc):\n",
    "    extractedData = dataExtractor.extract(pdf_doc)\n",
    "    RawDataDict = MergeData.mergeLinesToParagraphs(extractedData)\n",
    "    return RawDataDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GET STRUCTURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structRun(RawDataDict):\n",
    "    markers =       structAnalyzer.extract_markers(RawDataDict)\n",
    "    structures =    structAnalyzer.build_structures(markers)\n",
    "    dedup =         structAnalyzer.deduplicate(structures)\n",
    "    top =           structAnalyzer.select_top(dedup)\n",
    "    RawLvlsDict =   structAnalyzer.extend_top(top, dedup)\n",
    "    print(MU.json_convert(RawLvlsDict, pretty=True))\n",
    "    return RawLvlsDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BUILD CHUNKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkRun(RawLvlsDict=None, RawDataDict=None):\n",
    "    StructsDict = chunkBuilder.build(RawLvlsDict, RawDataDict)\n",
    "    return StructsDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BUILD SEGMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SegmentRun(StructsDict):\n",
    "    SegmentDict = [item for item in StructsDict if item.get(\"Level 1\", \"\").strip()]\n",
    "    for i, item in enumerate(SegmentDict, start=1): item[\"Index\"] = i\n",
    "    return SegmentDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCHEMA EXTRACTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schemaRun(SegmentDict):\n",
    "    SchemaDict = schemaExt.schemaRun(SegmentDict=SegmentDict)\n",
    "    print(SchemaDict)\n",
    "    return SchemaDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INDEXING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Indexing(SchemaDict):\n",
    "    Mapping, MapData = faissIndexer.build_from_json(\n",
    "        SegmentPath=SegmentPath,\n",
    "        SchemaDict=SchemaDict,\n",
    "        FaissPath=FaissPath,\n",
    "        MapDataPath=MapDataPath,\n",
    "        MappingPath=MappingPath,\n",
    "        MapChunkPath=MapChunkPath\n",
    "    )\n",
    "    return Mapping, MapData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RUN FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prepare():\n",
    "    print(\"\\nLoading File...\")\n",
    "    pdf_doc = fitz.open(PdfPath)\n",
    "\n",
    "    print(\"\\nExtracting...\")\n",
    "    RawDataDict = extractRun(pdf_doc)\n",
    "    MU.write_json(RawDataDict, RawDataPath, indent=1)\n",
    "\n",
    "    print(\"\\nGetting Struct...\")\n",
    "    RawLvlsDict = structRun(RawDataDict)\n",
    "    MU.write_json(RawLvlsDict, RawLvlsPath, indent=2)\n",
    "\n",
    "    print(\"\\nChunking...\")\n",
    "    StructsDict = chunkRun(RawLvlsDict, RawDataDict)\n",
    "    MU.write_json(StructsDict, StructsPath, indent=2)\n",
    "\n",
    "    print(\"\\nSegmenting...\")\n",
    "    SegmentDict = SegmentRun(StructsDict)\n",
    "    MU.write_json(SegmentDict, SegmentPath, indent=2)\n",
    "\n",
    "    print(\"\\nCreating Schema...\")\n",
    "    SchemaDict = schemaRun(SegmentDict)\n",
    "    MU.write_json(SchemaDict, SchemaPath, indent=2)\n",
    "\n",
    "    print(\"\\nEmbedding...\")\n",
    "    Mapping, MapData = Indexing(SchemaDict)\n",
    "    MU.write_json(Mapping, MappingPath, indent=2)\n",
    "    MU.write_json(MapData, MapDataPath, indent=2)\n",
    "\n",
    "    print(\"\\nCompleted!\")\n",
    "    pdf_doc.close()\n",
    "    return RawDataDict, RawLvlsDict, StructsDict, SegmentDict, SchemaDict, Mapping, MapData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RawDataDict, RawLvlsDict, StructsDict, SegmentDict, SchemaDict, Mapping, MapData = Prepare()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
