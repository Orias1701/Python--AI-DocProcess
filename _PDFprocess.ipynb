{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import fitz\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "from Config import Configs\n",
    "from Config import ModelLoader as ML\n",
    "from Libraries import Common_MyUtils as MU, Common_TextProcess as TP, Common_PdfProcess as PP\n",
    "from Libraries import PDF_QualityCheck as QualityCheck, PDF_ExtractData as ExtractData, PDF_MergeData as MergeData\n",
    "from Libraries import Json_ChunkUnder as ChunkUnder, Json_GetStructures as GetStructures, Json_ChunkMaster as ChunkMaster, Json_SchemaExt as SchemaExt\n",
    "from Libraries import Faiss_Embedding as F_Embedding, Faiss_Searching as F_Searching, Faiss_ChunkMapping as ChunkMapper\n",
    "from Libraries import Summarizer_Runner as SummaryRun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIGURATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HARD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = \"Categories\"\n",
    "infilename = \"HNMU\"\n",
    "JsonKey = \"paragraphs\"\n",
    "JsonField = \"Text\"\n",
    "\n",
    "MODEL_DIR = \"Models\"\n",
    "MODEL_SUMARY = \"Summarizer\"\n",
    "MODEL_ENCODE = \"Sentence_Transformer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOAD CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Configs.ConfigValues(pdfname=infilename, service=service)\n",
    "\n",
    "PdfPath = config[\"PdfPath\"]\n",
    "exceptPath = config[\"exceptPath\"]\n",
    "markerPath = config[\"markerPath\"]\n",
    "statusPath = config[\"statusPath\"]\n",
    "\n",
    "RawDataPath = config[\"RawDataPath\"]\n",
    "RawLvlsPath = config[\"RawLvlsPath\"]\n",
    "StructsPath = config[\"StructsPath\"]\n",
    "SegmentPath = config[\"SegmentPath\"]\n",
    "SchemaPath = config[\"SchemaPath\"]\n",
    "FaissPath = config[\"FaissPath\"]\n",
    "MappingPath = config[\"MappingPath\"]\n",
    "MapDataPath = config[\"MapDataPath\"]\n",
    "MapChunkPath = config[\"MapChunkPath\"]\n",
    "MetaPath = config[\"MetaPath\"]\n",
    "\n",
    "serviceSegmentPath = config[\"serviceSegmentPath\"]\n",
    "serviceFaissPath = config[\"serviceFaissPath\"]\n",
    "serviceMappingPath = config[\"serviceMappingPath\"]\n",
    "serviceMapDataPath = config[\"serviceMapDataPath\"]\n",
    "serviceMapChunkPath = config[\"serviceMapChunkPath\"]\n",
    "serviceMetaPath = config[\"serviceMetaPath\"]\n",
    "\n",
    "DATA_KEY = config[\"DATA_KEY\"]\n",
    "EMBE_KEY = config[\"EMBE_KEY\"]\n",
    "SEARCH_EGINE = config[\"SEARCH_EGINE\"]\n",
    "RERANK_MODEL = config[\"RERANK_MODEL\"]\n",
    "RESPON_MODEL = config[\"RESPON_MODEL\"]\n",
    "EMBEDD_MODEL = config[\"EMBEDD_MODEL\"]\n",
    "CHUNKS_MODEL = config[\"CHUNKS_MODEL\"]\n",
    "SUMARY_MODEL = config[\"SUMARY_MODEL\"]\n",
    "WORD_LIMIT = config[\"WORD_LIMIT\"]\n",
    "\n",
    "EMBEDD_CACHED_MODEL = f\"{MODEL_DIR}/{MODEL_ENCODE}/{EMBEDD_MODEL}\"\n",
    "CHUNKS_CACHED_MODEL = F\"{MODEL_DIR}/{MODEL_ENCODE}/{CHUNKS_MODEL}\"\n",
    "SUMARY_CACHED_MODEL = f\"{MODEL_DIR}/{MODEL_SUMARY}/{SUMARY_MODEL}\"\n",
    "\n",
    "MAX_INPUT = 1024\n",
    "MAX_TARGET = 256\n",
    "MIN_TARGET = 64\n",
    "TRAIN_EPOCHS = 3\n",
    "LEARNING_RATE = 3e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXCEPTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadHardcodes(file_path, wanted=None):\n",
    "    data = MU.read_json(file_path)\n",
    "    if \"items\" not in data:\n",
    "        return\n",
    "    result = {}\n",
    "    for item in data[\"items\"]:\n",
    "        key = item[\"key\"]\n",
    "        if (not wanted) or (key in wanted):\n",
    "            result[key] = item[\"values\"]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOAD EXCEPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptData = loadHardcodes(exceptPath, wanted=[\"common_words\", \"proper_names\", \"abbreviations\"])\n",
    "markerData = loadHardcodes(markerPath, wanted=[\"keywords\", \"markers\"])\n",
    "statusData = loadHardcodes(statusPath, wanted=[\"brackets\", \"sentence_ends\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loader = ML.ModelLoader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOAD MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer, embeddDevice = Loader.load_encoder(EMBEDD_MODEL, EMBEDD_CACHED_MODEL)\n",
    "chunker, chunksDevice = Loader.load_encoder(CHUNKS_MODEL, CHUNKS_CACHED_MODEL)\n",
    "\n",
    "tokenizer, summarizer, summaryDevice = Loader.load_summarizer(SUMARY_MODEL, SUMARY_CACHED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN FLOW CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXTRACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checker = QualityCheck.PDFQualityChecker()\n",
    "\n",
    "dataExtractor = ExtractData.B1Extractor(\n",
    "    exceptData,\n",
    "    markerData,\n",
    "    statusData,\n",
    "    proper_name_min_count=10\n",
    ")\n",
    "\n",
    "merger = MergeData.ParagraphMerger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STRUCT CHUNKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structAnalyzer = GetStructures.StructureAnalyzer(\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chunkBuilder = ChunkMaster.ChunkBuilder()\n",
    "\n",
    "schemaExt = SchemaExt.JSONSchemaExtractor(\n",
    "    list_policy=\"first\", \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INDEXER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faissIndexer = F_Embedding.DirectFaissIndexer(\n",
    "    indexer=indexer,\n",
    "    device=str(embeddDevice),\n",
    "    batch_size=32,\n",
    "    show_progress=True,\n",
    "    flatten_mode=\"split\",\n",
    "    join_sep=\"\\n\",\n",
    "    allowed_schema_types=(\"string\", \"array\", \"dict\"),\n",
    "    max_chars_per_text=2000,\n",
    "    normalize=True,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SEGMENT CHUNKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkUnder = ChunkUnder.ChunkUndertheseaBuilder(\n",
    "    embedder=indexer,\n",
    "    device=embeddDevice,\n",
    "    min_words=256,\n",
    "    max_words=768,\n",
    "    sim_threshold=0.7,\n",
    "    key_sent_ratio=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SUMMARIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryEngine = SummaryRun.RecursiveSummarizer(\n",
    "    tokenizer=tokenizer,\n",
    "    summarizer=summarizer,\n",
    "    sum_device=summaryDevice,\n",
    "    chunk_builder=chunkUnder,\n",
    "    max_length=200,\n",
    "    min_length=100,\n",
    "    max_depth=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SEARCHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = CrossEncoder(RERANK_MODEL, device=str(embeddDevice))\n",
    "searchEngine = F_Searching.SemanticSearchEngine(\n",
    "    indexer=indexer,\n",
    "    reranker=reranker,\n",
    "    device=str(embeddDevice),\n",
    "    normalize=True,\n",
    "    top_k=20,\n",
    "    rerank_k=10,\n",
    "    rerank_batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN FLOW FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHECKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdfCheck(pdf_doc):\n",
    "    is_good, metrics = checker.evaluate(pdf_doc)\n",
    "    return is_good, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXTRACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractRun(pdf_doc):\n",
    "    extractedData = dataExtractor.extract(pdf_doc)\n",
    "    RawDataDict = merger.merge(extractedData)\n",
    "    return RawDataDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROCESS FOR SEARCHING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STRUCT GETTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structRun(RawDataDict):\n",
    "    markers =       structAnalyzer.extract_markers(RawDataDict)\n",
    "    structures =    structAnalyzer.build_structures(markers)\n",
    "    dedup =         structAnalyzer.deduplicate(structures)\n",
    "    top =           structAnalyzer.select_top(dedup)\n",
    "    RawLvlsDict =   structAnalyzer.extend_top(top, dedup)\n",
    "    \n",
    "    print(MU.json_convert(RawLvlsDict, pretty=True))\n",
    "    return RawLvlsDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STRUCT CHUNKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkRun(RawLvlsDict=None, RawDataDict=None):\n",
    "    StructsDict = chunkBuilder.build(RawLvlsDict, RawDataDict)\n",
    "    return StructsDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SEGMENT CHUNKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SegmentRun(StructsDict, RawLvlsDict):\n",
    "    first_key = list(RawLvlsDict[0].keys())[0]\n",
    "\n",
    "    SegmentDict = []\n",
    "    for item in StructsDict:\n",
    "        value = item.get(first_key)\n",
    "        if not value:\n",
    "            continue\n",
    "        \n",
    "        if isinstance(value, list):\n",
    "            value = \" \".join(\n",
    "                v.strip() for v in value\n",
    "                if isinstance(v, str) and v.strip().lower() != \"none\"\n",
    "            )\n",
    "            if value.strip():\n",
    "                SegmentDict.append(item)\n",
    "\n",
    "        elif isinstance(value, str):\n",
    "            text = value.strip()\n",
    "            if text and text.lower() != \"none\":\n",
    "                SegmentDict.append(item)\n",
    "\n",
    "    for i, item in enumerate(SegmentDict, start=1):\n",
    "        item[\"Index\"] = i\n",
    "\n",
    "    return SegmentDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCHEMA GETTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schemaRun(SegmentDict):\n",
    "    SchemaDict = schemaExt.schemaRun(SegmentDict=SegmentDict)\n",
    "    print(SchemaDict)\n",
    "    return SchemaDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INDEXER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Indexing(SchemaDict):\n",
    "    FaissIndex, Mapping, MapData, chunk_groups = faissIndexer.build_from_json(\n",
    "        SegmentPath=SegmentPath,\n",
    "        SchemaDict=SchemaDict,\n",
    "        FaissPath=FaissPath,\n",
    "        MapDataPath=MapDataPath,\n",
    "        MappingPath=MappingPath,\n",
    "        MapChunkPath=MapChunkPath\n",
    "    )\n",
    "    return FaissIndex, Mapping, MapData, chunk_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROCESS FOR CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAW MERGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergebyText(RawDataDict):\n",
    "    merged_text = TP.merge_txt(RawDataDict, JsonKey, JsonField)\n",
    "    return merged_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SUMMARIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summaryRun(merged_text):\n",
    "    summarized = summaryEngine.summarize(merged_text, minInput = 256, maxInput = 1024)\n",
    "    return summarized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINAL PROCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SEARCHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runSearch(query, faissIndex, Mapping, MapData, MapChunk):\n",
    "    results = searchEngine.search(\n",
    "        query=query,\n",
    "        faissIndex=faissIndex,\n",
    "        Mapping=Mapping,\n",
    "        MapData=MapData,\n",
    "        MapChunk=MapChunk,\n",
    "        top_k=20\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RERANKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runRerank(query, results):\n",
    "    reranked = searchEngine.rerank(\n",
    "        query=query,\n",
    "        results=results,\n",
    "        top_k=10\n",
    "    )\n",
    "    return reranked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MERGED FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadData(SegmentPath, FaissPath, MappingPath, MapDataPath, MapChunkPath):\n",
    "    SegmentDict = MU.read_json(SegmentPath)\n",
    "    FaissIndex = faiss.read_index(FaissPath)\n",
    "    Mapping = MU.read_json(MappingPath)\n",
    "    MapData = MU.read_json(MapDataPath)\n",
    "    MapChunk = MU.read_json(MapChunkPath)\n",
    "    return {\n",
    "        \"SegmentDict\": SegmentDict,\n",
    "        \"FaissIndex\": FaissIndex,\n",
    "        \"Mapping\": Mapping,\n",
    "        \"MapData\": MapData,\n",
    "        \"MapChunk\": MapChunk\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### READ PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preReadPDF(PdfPath=None, PdfBytes=None):\n",
    "    if PdfBytes is not None:\n",
    "        pdf_doc = fitz.open(stream=PdfBytes, filetype=\"pdf\")\n",
    "    elif PdfPath is not None:\n",
    "        pdf_doc = fitz.open(PdfPath)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    checker = QualityCheck.PDFQualityChecker()\n",
    "    is_good, info = checker.evaluate(pdf_doc)\n",
    "    print(info)\n",
    "    if is_good:\n",
    "        print(\"✅ Tiếp tục xử lý.\")\n",
    "    else:\n",
    "        print(\"⚠️ Bỏ qua file này.\")\n",
    "        pdf_doc.close()\n",
    "        return None\n",
    "        \n",
    "    RawDataDict = extractRun(pdf_doc)\n",
    "    MU.write_json(RawDataDict, RawDataPath, indent=1)\n",
    "    pdf_doc.close()\n",
    "    \n",
    "    return RawDataDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREPARE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepareData(SegmentPath, FaissPath, MappingPath, MapDataPath, MapChunkPath, RawDataDict=None):            \n",
    "    if RawDataDict is not None:\n",
    "        RawLvlsDict = structRun(RawDataDict)\n",
    "        MU.write_json(RawLvlsDict, RawLvlsPath, indent=2)\n",
    "\n",
    "        StructsDict = chunkRun(RawLvlsDict, RawDataDict)\n",
    "        MU.write_json(StructsDict, StructsPath, indent=2)\n",
    "\n",
    "        SegmentDict = SegmentRun(StructsDict, RawLvlsDict)\n",
    "        MU.write_json(SegmentDict, SegmentPath, indent=2)\n",
    "        \n",
    "    elif MU.file_exists(SegmentPath):\n",
    "        SegmentDict = MU.read_json(SegmentPath)\n",
    "        \n",
    "    else :\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    SchemaDict = schemaRun(SegmentDict)\n",
    "    MU.write_json(SchemaDict, SchemaPath, indent=2)\n",
    "\n",
    "    FaissIndex, Mapping, MapData, chunk_groups = Indexing(SchemaDict)\n",
    "    MU.write_json(Mapping, MappingPath, indent=2)\n",
    "    MU.write_json(MapData, MapDataPath, indent=2)\n",
    "    \n",
    "    faiss.write_index(FaissIndex, FaissPath)\n",
    "    MU.write_chunkmap(MapChunkPath, SegmentPath, chunk_groups)\n",
    "    MapChunk = MU.read_json(MapChunkPath)\n",
    "    \n",
    "    print(\"\\nCompleted!\")\n",
    "    \n",
    "    return {\n",
    "        \"SegmentDict\": SegmentDict,\n",
    "        \"FaissIndex\": FaissIndex,\n",
    "        \"Mapping\": Mapping,\n",
    "        \"MapData\": MapData,\n",
    "        \"MapChunk\": MapChunk\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SUMMARIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizeDcmt(RawDataDict):\n",
    "    merged_text = mergebyText(RawDataDict)\n",
    "    summarized = summaryRun(merged_text)\n",
    "    return summarized[\"summary_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLASSIFY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyDocument(summaryText):\n",
    "    readedData = ReadData(serviceSegmentPath, serviceFaissPath, serviceMappingPath, serviceMapDataPath, serviceMapChunkPath)\n",
    "    serviceSegmentDict = readedData.get(\"SegmentDict\")\n",
    "    serviceFaissIndex = readedData.get(\"FaissIndex\")\n",
    "    serviceMapping = readedData.get(\"Mapping\")\n",
    "    serviceMapData = readedData.get(\"MapData\")\n",
    "    serviceMapChunk = readedData.get(\"MapChunk\")\n",
    "    \n",
    "    resuls = runSearch(summaryText, serviceFaissIndex, serviceMapping, serviceMapData, serviceMapChunk)\n",
    "    reranked = runRerank(summaryText, resuls)\n",
    "    \n",
    "    bestCategory = ChunkMapper.process_chunks_pipeline(reranked_results=reranked, SegmentDict=serviceSegmentDict, drop_fields=[\"Index\"], fields=[\"Article\"], n_chunks=1)\n",
    "    bestArticles = [item[\"fields\"].get(\"Article\") for item in bestCategory[\"extracted_fields\"]]\n",
    "    bestArticle = bestArticles[0] if len(bestArticles) == 1 else \", \".join(bestArticles)\n",
    "    return bestArticle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainRun(PdfPath=None, PdfBytes=None, queryText = \"Nhiệm vụ của sinh viên là gì?\"):\n",
    "    summaryText = \"\"\n",
    "    bestArticle = \"\"\n",
    "    response = \"\"\n",
    "    \n",
    "    RawDataDict = preReadPDF(PdfPath, PdfBytes)\n",
    "    \n",
    "    # summaryText = summarizeDcmt(RawDataDict)\n",
    "    # bestArticle = classifyDocument(summaryText)\n",
    "    \n",
    "    # preparedData = PrepareData(SegmentPath, FaissPath, MappingPath, MapDataPath, MapChunkPath, RawDataDict)\n",
    "    # SegmentDict=preparedData.get(\"SegmentDict\")\n",
    "    # FaissIndex=preparedData.get(\"FaissIndex\")\n",
    "    # Mapping=preparedData.get(\"Mapping\")\n",
    "    # MapData=preparedData.get(\"MapData\")\n",
    "    # MapChunk=preparedData.get(\"MapChunk\")\n",
    "    \n",
    "    readedData = ReadData(SegmentPath, FaissPath, MappingPath, MapDataPath, MapChunkPath)\n",
    "    SegmentDict = readedData.get(\"SegmentDict\")\n",
    "    FaissIndex = readedData.get(\"FaissIndex\")\n",
    "    Mapping = readedData.get(\"Mapping\")\n",
    "    MapData = readedData.get(\"MapData\")\n",
    "    MapChunk = readedData.get(\"MapChunk\")\n",
    "    \n",
    "    \n",
    "    resuls = runSearch(queryText, FaissIndex, Mapping, MapData, MapChunk)\n",
    "    reranked = runRerank(queryText, resuls)\n",
    "    chunkReturn = ChunkMapper.process_chunks_pipeline(\n",
    "        reranked_results=reranked,\n",
    "        SegmentDict=SegmentDict,\n",
    "        drop_fields=[\"Index\"],          # 1) Trường bị bỏ qua (áp dụng toàn bộ). None → không bỏ\n",
    "        fields=[\"Article\"],             # 2) Trường muốn trả cho mỗi chunk. None → tất cả top-level còn lại\n",
    "        n_chunks=1,                     # 3) Số lượng chunk gốc được trả về. None → tất cả\n",
    "    )\n",
    "    print(chunkReturn[\"chunks_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryText = \"Sinh viên được xét và công nhận tốt nghiệp khi có đủ các điều kiện nào?\"\n",
    "mainRun(PdfPath=PdfPath, PdfBytes=None, queryText=queryText)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bruh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
